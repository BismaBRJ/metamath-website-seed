<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html><head><title>AsteroidMeta: metamathMathQuestions</title><link type="text/css" rel="stylesheet" href="metamathMathQuestions_files/wiki.css"><meta name="robots" content="INDEX,NOFOLLOW"><link rel="alternate" type="application/rss+xml" title="AsteroidMeta" href="http://planetx.cc.vt.edu/AsteroidMeta?action=rss"><link rel="alternate" type="application/rss+xml" title="AsteroidMeta: metamathMathQuestions" href="http://planetx.cc.vt.edu/AsteroidMeta?action=rss;rcidonly=metamathMathQuestions"></head>
<body class="http://planetx.cc.vt.edu/AsteroidMeta"><div class="header"><span class="gotobar bar"><a class="local" href="HomePage">HomePage</a> <a class="local" href="RecentChanges">RecentChanges</a> </span><h1><a title="Click to search for references to this page" href="http://planetx.cc.vt.edu/AsteroidMeta?search=metamathMathQuestions">metamathMathQuestions</a></h1></div><div class="content browse"><p>Hopefully, Norman Megill will wander by and answer <a class="local" href="metamath">metamath</a> questions :)</p><hr><h2>Appendix 4: A Note on Definitions</h2><p><b>Question</b></p><p>I don't really understand what Norm is saying in <a class="url" href="http://us.metamath.org/mpegif/mmset.html">http://us.metamath.org/mpegif/mmset.html</a>, Appendix 4, about soundness. I'm not really sure what soundness checking is.</p><p><b>Answer</b></p><p>I added an example to Appendix 4 that hopefully makes this clearer. --<a class="local" href="norm">norm</a> 5 Sep 2005</p><hr><h2>Proper Substitution</h2><p><b>Question</b></p><p>How
the substitution definition has been coined ? What was the (potential)
intellectual process of its creator when he/she invented the
definition. I think it can be helpful to grasp a definition that is not
as straightforward as the usual one.</p><p><b>Answer</b></p><p>We are interested in "the wff that results when y is properly substituted for x in the wff phi," which we denote by [y/x]phi.</p><p>As
opposed to "simple substitution" (like Metamath's built-in rule), which
blindly replaces variables throughout an expression, "proper
substitution" pays attention to their context, i.e. whether they are
free or bound at a particular symbol position in the expression, and
substitutes only the free occurrences, renaming bound variables when
necessary to avoid clashes. For example, the proper substitution of y
for x in</p><pre>  (x = y /\ A.x x = y /\ E.y y = x)</pre><p>would be</p><pre>  (y = y /\ A.x x = y /\ E.q q = y),</pre><p>whereas simple substitution would result in</p><pre>  (y = y /\ A.y y = y /\ E.y y = y).</pre><p>These
have quite different logical meanings. In logic books, proper
substitution is typically presented as a list of mechanical wff
construction rules that cover the various cases. These mechanical rules
are fine for informal work, where with experience one learns to "see"
the proper substutition all at once, but in a formal proof (at the
extremely detailed level of Metamath) they can be tedious and
distracting.</p><p>There is an alternate method for representing proper
substitution that is much more compact. The idea is not to construct
the substitution instance explicitly according to these rules, but
instead to come up with a wff that is <i>logically equivalent</i>. In
the end, that is all we care about anyway. I'm not sure who originated
this method historically, but it is given in Quine's <i>Set Theory and Its Logic</i>.</p><p>The
basic idea is to notice that we can prove, as a theorem scheme of
standard predicate calculus, the following equivalence when x and y are
distinct (<a class="url" href="http://us.metamath.org/mpegif/sb5.html">http://us.metamath.org/mpegif/sb5.html</a>):</p><p>(Note:
I am being lazy by just copy/pasting from Metamath web pages; Mozilla
will paste the ASCII versions of the symbols into a text editor - a
very cool feature, by the way, that Internet Explorer doesn't have.
Even though it does not conform to Metamath's white space requirements,
I think it's reasonably readable.)</p><pre>  sb5     |- ([y / x]ph &lt;-&gt; E.x(x = y /\ ph))
  Distinct variable group(s):   x,y</pre><p>This
has the drawback of not working when x and y are the same; we the need
proviso $d x y. But with a third auxilliary variable z, we can prove
(as shown in Quine) (<a class="url" href="http://us.metamath.org/mpegif/sb7.html">http://us.metamath.org/mpegif/sb7.html</a>):</p><pre>  sb7     |- ([y / x]ph &lt;-&gt; E.z(z = y /\ E.x(x = z /\ ph)))
  Distinct variable group(s):   x,z   y,z   ph,z</pre><p>which works even when x and y are substituted with the same variable.</p><p>This
still has the drawback of requiring that the third "dummy" variable z
be distinct from both x and y as well as not occurring in ph. In
set.mm, I wanted to see how far we can go without ever requiring
variables to be distinct, i.e. before the $d statement is introduced.
(The answer is that it is quite far; in fact all of mathematics can be
done without the $d statement, subject to limitations discussed at <a class="url" href="http://us.metamath.org/mpegif/mmzfcnd.html">http://us.metamath.org/mpegif/mmzfcnd.html</a>.)
In order to do this, I needed a definition of proper substitution that
doesn't depend on the $d statement, and by playing around I found the
following equivalence for proper substitution, with <i>no restrictions at all</i> on x, y, or ph (<a class="url" href="http://us.metamath.org/mpegif/df-sb.html">http://us.metamath.org/mpegif/df-sb.html</a>):</p><pre>  df-sb   |- ([y / x]ph &lt;-&gt; ((x = y -&gt; ph) /\ E.x(x = y /\ ph)))</pre><p>The
trick is to have x free in the first conjunct and bound in the second,
in order to make it work both when x and y are the same and when they
are different. Personally, I think this is a very neat trick! However,
mixing free and bound variables in this way horrifies many logicians (I
will omit tales of their reactions when encountering this) and thus it
will probably never find its way outside of Metamath.</p><p>Additional discussion can be found at <a class="url" href="http://us.metamath.org/mpegif/df-sb.html">http://us.metamath.org/mpegif/df-sb.html</a>,
which of course has the complete formal proofs of the equivalences of
the various definitions above. But if you are still confused by df-sb
let me know and I'll try to explain it informally.</p><p>--<a class="local" href="norm">norm</a> 25-Sep-05</p><hr><h2>Proper Substitution (Cont.)</h2><p><b>Question</b></p><p>Thank
you Norm for this answer. It is an important step for me to some deeper
understanding of this definition. I'm still wondering why in sb5 the
distinct variable proviso is needed. Obviously the demonstration make
it mandatory but when I try to find examples, it seems to me that the
proviso is not needed. For example if we decide that ph is the
proposition x = x in this case y = y is equivalent to E. x ( ( x = y )
/\ x = x ) . And for this example the distinct variable proviso would
not be needed. I think that I have to find a more complex example (
with quantifier I imagine) to fully understand the necessity of the
proviso.</p><p>--<a class="local" href="frl">frl</a> 26-Sep-05</p><p><b>Answer</b></p><p>Recall sb5 (<a class="url" href="http://us.metamath.org/mpegif/sb5.html">http://us.metamath.org/mpegif/sb5.html</a>).</p><pre>  sb5     |- ([y / x]ph &lt;-&gt; E.x(x = y /\ ph))
  Distinct variable group(s):   x,y</pre><p>Suppose we violate the proviso and substitute x for both variables. In other words, suppose we claim:</p><pre>    |- ([x / x]ph &lt;-&gt; E.x(x = x /\ ph))</pre><p>Hopefully you will agree that [x / x]ph should be equivalent to ph (<a class="url" href="http://us.metamath.org/mpegif/sbid.html">http://us.metamath.org/mpegif/sbid.html</a>). So we get</p><pre>    |- (ph &lt;-&gt; E.x(x = x /\ ph))</pre><p>Also, x=x is always true (<a class="url" href="http://us.metamath.org/mpegif/equid.html">http://us.metamath.org/mpegif/equid.html</a>), so it's redundant and we can eliminate it (<a class="url" href="http://us.metamath.org/mpegif/biantrur.html">http://us.metamath.org/mpegif/biantrur.html</a>).</p><pre>    |- (ph &lt;-&gt; E.x ph)</pre><p>Let ph be x=z, where z is distinct from x.</p><pre>    |- (x=z &lt;-&gt; E.x x=z)</pre><p>Then the rhs E.x x=z is true (<a class="url" href="http://us.metamath.org/mpegif/a9e.html">http://us.metamath.org/mpegif/a9e.html</a>), and we can detach it (<a class="url" href="http://us.metamath.org/mpegif/mpbir.html">http://us.metamath.org/mpegif/mpbir.html</a>) to conclude</p><pre>    |- x=z</pre><p>Quantifying with generalization (<a class="url" href="http://us.metamath.org/mpegif/ax-gen.html">http://us.metamath.org/mpegif/ax-gen.html</a>) we get</p><pre>   |- A.x x=z</pre><p>which is false in set theory (<a class="url" href="http://us.metamath.org/mpegif/dtru.html">http://us.metamath.org/mpegif/dtru.html</a>), giving us a contradiction.</p><p>I
hope I have given enough detail so that, by restating the proviso-free
sb5 as an axiom, you should be able to prove this contradiction as a
Metamath exercise. Indeed, by detaching the contradictory statements
from the antecedents of the Duns Scotus law (<a class="url" href="http://us.metamath.org/mpegif/pm2.21.html">http://us.metamath.org/mpegif/pm2.21.html</a>)
you can instantly prove anything from Fermat's Last Theorem (or its
negation!) to the Goldbach Conjecture to impress your friends. :)</p><p>This
example shows how fragile a mathematical proof can be. A tiny, barely
noticeable oversight such as overlooking a proviso can lead to
inconsistency. Imagine if such a subtle defect were hidden deep inside
of a long, important proof! The entire proof, and any further results
based on it, would collapse into a pile of dust. The elimination of
such a possibility is a reason automated proof verification can be
useful.</p><p>By the way, you cannot show a contradiction from the
proviso-free sb5 using predicate calculus alone, which may explain why
you were puzzled by it. The reason is that predicate calculus is
intended to apply to any theory with a non-empty domain of discourse.
In the case of a trivial theory with a single element in its domain,
the proviso-free sb5 would hold; in fact it could be used as an axiom
for such a theory.</p><p>--<a class="local" href="norm">norm</a> 26-Sep-05</p><hr><h2>hbth : no variable is (effectively) free in a theorem</h2><p><b>Question</b></p><p>This tautology in metamath says that <em>|- ph =&gt; |- ( ph -> A. x ph )</em>. That's a very strange theorem because the conclusion is the metamath way to say that <em>x</em> is bound in <em>ph</em>. And we conclude from this theorem that <em>x</em> is bound in  <em>x = x</em>. Obviously outside from metamath, when we use the usual definition of a bound variable, <em>x</em> is ( according to me ) free in <em>x = x</em>. That's a point on which metamath differs from the usual construction of the bound property.</p><p>1) In the comment of hbth Norman says <em>no variable is (effectively) free in a theorem</em>.
I wonder why he added "effectively". Does it refer to the curious
property described above ? 2) Are there other cases than the one cited
above where the metamath definition really differs from the usual
concept of binding ?</p><p><b>Answer</b></p><p>Norm answers -</p><p>1) The word "effectively" was explained in an older version of the Metamath Proof Explorer Home Page <a class="url" href="http://de2.metamath.org/mpegif/mmset.html#traditional">http://de2.metamath.org/mpegif/mmset.html#traditional</a>, which says</p><dl><dt></dt><dd>Metamath's axiom system has no built-in notion of free and bound variables. Instead, we use the hypothesis <em>(ph -> A.x ph)</em> when we want to say (as we do in stdpc5) that "<em>x</em> is effectively not free in <em>ph</em>." We say "effectively" because any wff that is also a theorem, such as <em>x = x</em>, will satisfy the hypothesis even though x may technically be free in it.</dd></dl><p>I took this out in a rewrite of that section to reduce the amount of text. Perhaps I should put it back. The current version <a class="url" href="http://us.metamath.org/mpegif/mmset.html#traditional">http://us.metamath.org/mpegif/mmset.html#traditional</a> says</p><dl><dt></dt><dd>Metamath
does not have the traditional system's notions of "(does) not (occur)
free in" and "free for" built-in. However, we can emulate these notions
(in either system) with conventions that are based on a formula's <em>logical</em> properties (rather than its <em>structure</em>, as is the case with the traditional axioms). To say that "<em>x</em> is (effectively) not free in <em>ph</em>," we can use the hypothesis <em>ph -> A.x ph</em>. This hypothesis holds in all cases where <em>x</em> does not occur free in <em>ph</em> in the traditional system (and in a few other cases as well).</dd></dl><p>This
is not to say that Metamath can't emulate the traditional technical
definition, as you are doing in nat.mm, but only that for my axiom
system I chose not to, in order to keep the primitive starting concepts
(as opposed to derived high-level concepts) as simple as possible.</p><p>By the way, although <em>(ph -> A.x ph)</em> works as a hypothesis, it will not always work as an antecedent in a formula, since <em>x</em> may be (effectively) free in it. Instead, <em>A. x (ph -> A.x ph)</em>
will always work both as a hypothesis and as an antecedent in order to
denote "(effectively) not free". The first quantifier is usually
removed for simplicity in hypotheses since ax-gen can put it back.</p><p>2) Another curious example is hbae, which shows that <em>y</em> is (effectively) not free in  <em>A. x x = y</em>. Neither <em>A. x x = y</em> nor its negation is a theorem of predicate calculus.</p><p>--<a class="local" href="norm">norm</a> 9 Oct 2005</p><p>A third curious example is obtained by combining <a class="url outside" href="http://us2.metamath.org:8888/mpegif/hbs1.html">hbs1</a> and <a class="url outside" href="http://us2.metamath.org:8888/mpegif/df-sb.html">df-sb</a>:</p><pre>       ( ( ( x = y -&gt; ph ) /\ E. x ( x = y /\ ph ) )
                -&gt; A. x ( ( x = y -&gt; ph ) /\ E. x ( x = y /\ ph ) ) )</pre><p>when
x and y are distinct. This, of course, says that x is effectively not
free in [ y / x ] ph when x and y are distinct. But notice that in ( x
= y -> ph ), not only is x free in x = y, it may also be free in ph! So
this is an example where "effectively not free" and "not free" in the
traditional sense are quite different. This property, by the way, seems
to be intimately tied to <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ax-11.html">ax-11</a> from which it is derived. In particular, when x and y are distinct, the derivative <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ax11a.html">ax11a</a> gives us</p><pre>  ( x = y -&gt; ( ph -&gt; A. x ( x = y -&gt; ph ) ) )</pre><p>which somewhat counterintuitively quantifies a variable in the consequent that is free in the antecedents.</p><p>--<a class="local" href="norm">norm</a> 17-Dec-2005</p><hr><h2>Restatement of $d as Metamath formula possible?</h2><p><b>Question</b></p><p>Heretofore
I have viewed the $d statement as outside of the Metamath grammar, as
if it is not actually part of the user-defined language but is somehow
outside of it. In a sense, that is true because it only comes into play
inside proofs, but now the question of converting arbitrary .mm file(s)
to/from other "mathelogical" systems is of more widespread interest,
and a mechanical method of mapping $d semantics is desired (which may
not be easy to specify?).</p><p>Is it possible to convert each $d statement to one or more $e logical hypotheses that <b>exactly</b> capture the meaning of the original $d restriction?</p><p>--<a class="local" href="ocat">ocat</a></p><p><b>Answer</b></p><p>Although
I'm not sure what you mean by "exactly," a $d can be converted to a $e
with a "distinctor" (see below), with an important catch. The problem
is that when the $d is no longer needed, its $e emulation can't be made
to vanish, unlike the $d.</p><p>Let me describe the situation in more
detail. $d x y can be emulated with the hypothesis or antecedent -. A.
x x = y, called (by me) a "distinctor". If x and y are distinct, it is
true. If they are the same, it is false. Theorem <a class="url outside" href="http://us.metamath.org/mpegif/alequcom.html">alequcom</a> shows that the order of x and y doesn't matter; -. A. x x = y and -. A. y y = x are equivalent. By virtue of <a class="url outside" href="http://us.metamath.org/mpegif/hbnae.html">hbnae</a>, a distinctor (effectively) has no free variables, so <a class="url outside" href="http://us.metamath.org/mpegif/19.21ai.html">19.21ai</a> can be used to emulate generalization when you have the conjunction of a bunch of distinctors hanging around as an antecedent.</p><p>The "$d-free fragment" of predicate calculus <a class="url outside" href="http://us.metamath.org/mpegif/mmset.html#pcaxioms">through ax-15,
omitting ax-16,</a>
is a complete system of logic, with the important exception that as
many distinctors will accumulate as there are dummy variables required
by the proof (in addition to any distinctors that are required by the
theorem itself). Often distinctors can be minimized or eliminated with
tricks like the Distinctor Reduction Theorem (9.4) of my <a class="url outside" href="http://us.metamath.org/mpegif/mmset.html#bibmegill">paper</a>.</p><p>However,
in the end we will ultimately be defeated by a deep theorem of Andréka,
which shows that there is no upper bound on the number of dummy
variables that may be needed in an arbitrary proof, no matter what
tricks you use or how clever you are. This theorem is the real reason
we need $d's or some equivalent method to get rid of dummy variables.
It means that it is theoretically impossible to avoid the eventual
accumulation of distinctors if we don't have such a method.</p><p>(So
far I haven't found an actual theorem that has a redundant distinctor
that can't be eliminated, as should exist per Andréka's theorem, but
that is probably because I haven't played around enough in $d-free
world. The only thing I've done in that world is to prove the 200
$d-free predicate calculus theorems in set.mm, to obtain the
completeness results for the $d-free system for my paper, and to prove
$d-free versions of the ZFC axioms. None ever showed up that I couldn't
eliminate with various tricks. Who knows, an actual uneliminable
redundant distinctor may be one of those elusive things like sentences
of arithmetic that are true but not provable, that sort of thing. Or it
may be more common. But you can be sure that it's out there, somewhere!
Andréka's theorem says so!)</p><p>If you use a distinctor in a $e
instead of an antecedent, there is another problem (in set.mm, although
nat.mm could overcome this). You can't convert it back to an antecedent
with the <a class="url outside" href="http://us.metamath.org/mpegif/mmdeduction.html">weak deduction theorem</a>
because it is never true in predicate calculus. This prevents using
tricks such as Distinctor Reduction Theorem. So they will linger
forever as $e's in a proof, even when it is otherwise theoretically
possible to eliminate them. Eventually they could be eliminated with <a class="url outside" href="http://us.metamath.org/mpegif/dtru.html">dtru</a>,
although that requires introducing the $d you want to avoid.
Alternately, you could have an artificial (but sound) rule that
discards a distinctor in a $e when one of its variables is not in the
theorem or its non-distinctor $e's.</p><p>On the other hand, if we are
willing to tolerate the accumulation of distinctors, perhaps hiding
them for a human display of the theorem, it is possible to do all of
set theory without the $d statement! This is discussed at <a class="url outside" href="http://us.metamath.org/mpegif/mmzfcnd.html">ZFC Axioms
Without Distinct Variable Conditions</a>. I think that is remarkable.</p><p>So far, I have talked about replacing <img class="InlineMath" src="metamathMathQuestions_files/TAtr0ie6vAmKgR0IGRMppQ.png" alt="$ d x y, not  $">d x ph (which arises from the very useful, although theoretically redundant, <a class="url outside" href="http://us.metamath.org/mpegif/ax-17.html">ax-17</a>).
The $d x ph can easily be emulated with the (weaker) hypothesis $e |- (
ph -> A. x ph ), or alternately by replacing ph with A. x ph throughout
the theorem in order to "protect" it from x. Note that the hypothesis
$e |- ( ph -> A. x ph ) is actually just a restatement of ax-17 without
its $d requirement.</p><p>--<a class="local" href="norm">norm</a> 15 Oct 2005</p><p>Thanks!</p><p>I feared as much – and a mechanical conversion from <img class="InlineMath" src="metamathMathQuestions_files/X3QS6ElmoZTOerqMiLalaQ.png" alt="$ d to  $">e would involve reworking the proofs.</p><p><a class="url" href="http://www.gutenberg.org/dirs/etext91/lglass19.zip">http://www.gutenberg.org/dirs/etext91/lglass19.zip</a></p><pre>    The Queen propped her up against a tree, and said kindly,
    'You may rest a little now.'</pre><pre>    Alice looked round her in great surprise.  'Why, I do believe
    we've been under this tree the whole time!  Everything's just
    as it was!'</pre><pre>    'Of course it is,' said the Queen, 'what would you have it?'</pre><pre>    'Well, in OUR country,' said Alice, still panting a little,
    'you'd generally get to somewhere else--if you ran very fast
    for a long time, as we've been doing.'</pre><pre>    'A slow sort of country!' said the Queen.  'Now, HERE, you see,
    it takes all the running YOU can do, to keep in the same place.
    If you want to get somewhere else, you must run at least twice as
    fast as that!'</pre><p>--<a class="local" href="ocat">ocat</a> 15-Oct-2005 </p><hr><h2>Superfluous optional $d statements?</h2><p><b>Question</b> (emailed to me)</p><p>There's something I don't understand about $d statements. Take for example ddeeq1 from set.mm:</p><pre>  ${
    $d w z x $.  $d w y $.
    $( Quantifier introduction when one pair of variables is distinct. $)
    ddeeq1 $p |- ( -. A. x x = y -&gt; ( y = z -&gt; A. x y = z ) ) $=
      ( vw weq ax-17 a8b ddelim ) DCEZBCEABDIAFDBCGH $.
      $( [2-Jan-02] $)
  $}</pre><p>Here there are three optional $d restrictions (w is disjoint with x, y, and z) and one mandatory one (x and z are disjoint).</p><p>But
from the reader's point of view, the optional restrictions are really
superfluous. They're just an artifact of the way the proof is
structured. So these three restrictions are really part of the proof,
not of the assertion. Another way to see this: If this were an axiom,
then it would suffice to have $d z y $.</p><p>Why is it necessary to
state them explicitly? How is it possible to prove a wrong theorem when
such optional restrictions are left out? Or perhaps that isn't
possible, but this is just to help the verify algorithm?</p><p><b>Answer</b></p><dl class="quote"><dt></dt><dd><em>But from the reader's point of view, the optional restrictions are really superfluous.</em></dd></dl><p>For
most readers, yes. You may have noticed they are suppressed in the web
page versions of the proofs for this reason, because including them can
make the "distinct variable groups" cluttered and confusing.</p><dl class="quote"><dt></dt><dd><em>They're
just an artifact of the way the proof is structured. So these three
restrictions are really part of the proof, not of the assertion.
Another way to see this: If this were an axiom, then it would suffice
to have $d z y $.</em></dd></dl><p>Correct.</p><dl class="quote"><dt></dt><dd><em>Why is it necessary to state them explicitly?</em></dd></dl><p>Theoretically it is not necessary, and we could just assume that all dummy variables are distinct from all other variables.</p><p>But
there can be a minor benefit for the reader. If the $d's for dummy
variables were hidden, the reader would have no way of telling by
inspection what adjustments could be made to the proof. For example,
the proof might have dummy variables v and w that must be distinct from
the the theorems' variables but not from each other. Therefore we could
change both of them to w to reduce the number of hypotheses and
possibly shorten compressed proofs by increasing the likelihood of
common subexpressions. Or we could leave it alone if we prefer the
better readibility of separate v and w, that may serve two different
purposes in the proof.</p><p>There are also proofs (e.g. in
propositional calculus) where dummy variables are used in the proof for
clarity. For example, look at step 2 of <a class="url" href="http://us.metamath.org/mpegif/biigb.html">http://us.metamath.org/mpegif/biigb.html</a>.
Since there is no $d in set.mm, the variables χ (chi) and θ (theta) can
be changed to whatever you want, either for best readability or for
minimum number of hypotheses. (A person reading the web page wouldn't
know this, only someone reading set.mm.)</p><dl class="quote"><dt></dt><dd><em>How is it possible to prove a wrong theorem when such optional restrictions are left out?</em></dd></dl><p>If
you are talking about a hypothetical program that automatically assumes
$d for dummy variables, then it shouldn't be possible. Everything
should be fine.</p><dl class="quote"><dt></dt><dd><em>Or perhaps that isn't possible, but this is just to help the verify algorithm?</em></dd></dl><p>It
can make proof verification a little easier, but that is a relatively
minor issue. I think it might also make the spec slightly more
straightforward, by not having to describe implicit $d's for dummy
variables, although it is arguably a toss-up between that and having to
describe "optional $d restrictions".</p><p>I might also mention that in theory <em>all</em>
$d's except those accompanying axioms are superfluous, since the proof
verifier could use the violations to reconstruct what they should be.
Of course it would be compute-intensive to determine them this way,
since proofs could not be verified independently from each other.</p><p>The
bottom line is, if you are designing a new language based on Metamath,
whether you require dummy variables to have distinct variable
requirements is largely a matter of taste. Ideally they should be put
back automatically when converting to back to Metamath .mm format, but
that can also be done with a simple script based on what the failure
are (see the next question).</p><p><a class="local" href="norm">norm</a> 16-Oct-05</p><hr><h2>Metamath tips and techniques: creating $d statements</h2><p><b>Question</b></p><p>What is an easier way to figure out what $d requirements must accompany a complex theorem?</p><p><b>Answer</b></p><p>Sometimes
theorems can have rather complex distinct variable requirements. Rather
than spend time manually figuring them out, I will often just prove the
theorem without them and then use the metamath program's "verify proof"
error checking to tell me what they should be. For example, if you
remove the $d's from cplem2 in set.mm, "verify proof cplem2" will
produce error messages starting with:</p><pre>  ?Error on line 39915 of file "set.mm" at statement 13860, label
  "cplem2", type "$p":
      eqid cplem1 cB c0 wceq wn cB vy cv cin c0 wceq wn wi vx cA wral cB
           ^^^^^^
  There is a disjoint variable ($d) violation at proof step 122.
  Assertion "cplem1" requires that variables "x" and "y" be disjoint.
  But "x" was substituted with "x" and "y" was substituted with "z".
  Variables "x" and "z" do not have a disjoint variable requirement in
  the assertion being proved, "cplem2".</pre><p>There
are 546 lines of such error messages, with duplicate violations (due to
violating more than one referenced theorem) that make them tedious to
wade through. However, if you have Linux or Cygwin on Windows, you can
type</p><pre>  $ ./metamath 'r set.mm' 'v p cplem2' q | grep 'have' | sort | uniq</pre><p>yielding</p><pre>  Variables "A" and "w" do not have a disjoint variable requirement in
  Variables "A" and "x" do not have a disjoint variable requirement in
  Variables "A" and "y" do not have a disjoint variable requirement in
  Variables "A" and "z" do not have a disjoint variable requirement in
  Variables "B" and "w" do not have a disjoint variable requirement in
  Variables "B" and "y" do not have a disjoint variable requirement in
  Variables "B" and "z" do not have a disjoint variable requirement in
  Variables "w" and "x" do not have a disjoint variable requirement in
  Variables "w" and "y" do not have a disjoint variable requirement in
  Variables "w" and "z" do not have a disjoint variable requirement in
  Variables "x" and "y" do not have a disjoint variable requirement in
  Variables "x" and "z" do not have a disjoint variable requirement in
  Variables "y" and "z" do not have a disjoint variable requirement in</pre><p>which can easily be translated manually to the $d statements</p><pre>    $d A w x y z $.  $d B w y z $.</pre><p>Of course, in an ideal world all of this would be automated inside the Proof Assistant. But that is a topic for another day…</p><p>--<a class="local" href="norm">norm</a> 6-Oct-05</p><hr><p>If you don't have Linux or Cygwin, you can emulate the above $d filtering command</p><pre>  $ ./metamath 'r set.mm' 'v p cplem2' q | grep 'have' | sort | uniq</pre><p>as
follows, using the "tools" facility of the metamath program. Type this
sequence of commands into the metamath program, or put them into a
script which can be run with metamath's "submit" command.</p><pre>  read set.mm
  open log 1.tmp
  verify proof cplem2
  close log
  tools
  match 1.tmp 'have' ''
  unduplicate 1.tmp
  type 1.tmp 100
  exit</pre><p>--<a class="local" href="norm">norm</a> 10-Oct-05</p><hr><h2>Guidelines for adding and removing $d's in proofs in set.mm</h2><p><b>Question</b></p><p>How
do I convert $d's (distinct variable requirements) to $d-free
hypotheses, and vice-versa, when creating proofs in set.mm? How do I
get rid of distinctors (antecedent of the form -. A.x x=y) that are
theoretically unnecessary?</p><p><b>Answer</b></p><p>Below I list the
principle guidelines for dealing with $d's.All of these are implicitly
part of the proof of the metalogical completeness theorem in my paper,
but I think the practical examples below will be more helpful than some
abstract theory.</p><dl class="quote"><dt></dt><dd>1. Converting $d x y to distinctor - use <a class="url outside" href="http://us.metamath.org/mpegif/dvelim.html">dvelim</a>. (It is more of an art than a science at this point, since I don't use it very often, but go by the examples of its usage.)</dd><dt></dt><dd>2. Converting distinctor to $d x y - use mainly <a class="url outside" href="http://us.metamath.org/mpegif/ax-16.html">ax-16</a>; see e.g. <a class="url outside" href="http://us.metamath.org/mpegif/ax17eq.html">ax17eq</a> and other uses of ax-16. If you have set theory, it's trivial; use <a class="url outside" href="http://us.metamath.org/mpegif/dtru.html">dtru</a>. But it's not hard without dtru either, so don't add set theory just to get dtru. :)</dd><dt></dt><dd>3. Converting $d x ph to (ph -> A.x ph) hypothesis - usually done via a dummy variable and <a class="url outside" href="http://us.metamath.org/mpegif/cbval.html">cbval</a>, <a class="url outside" href="http://us.metamath.org/mpegif/cbvex.html">cbvex</a>; see e.g. <a class="url outside" href="http://us.metamath.org/mpegif/df-eu.html">df-eu</a> -&gt; <a class="url outside" href="http://us.metamath.org/mpegif/euf.html">euf</a>.<dl class="quote"><dt></dt><dd>I'd
like to say to the pure and young metamathician that in fact the $d
statement doesn't disappear; it is only transfered to a dummy variable.
I want to say that because I wondered where the $d statement had gone
for a while since the $d statement about dummy variables are not
mentioned on the html pages. <a class="local" href="frl">frl</a> 11-Nov-2005</dd></dl></dd><dt></dt><dd>4. Converting (ph -> A.x ph) hypothesis to $d x ph: trivial; use <a class="url outside" href="http://us.metamath.org/mpegif/ax-17.html">ax-17</a></dd><dt></dt><dd>5. Eliminating unnecessary distinctors - use <a class="url outside" href="http://us.metamath.org/mpegif/pm2.61i.html">pm2.61i</a> together with dr* theorems e.g. <a class="url outside" href="http://us.metamath.org/mpegif/dral1.html">dral1</a>; see the proof of <a class="url outside" href="http://us.metamath.org/mpegif/sb9i.html">sb9i</a> for a good example of this.</dd></dl><p>--<a class="local" href="norm">norm</a> 25-Oct-2005</p><hr><h2>Logical vs. metalogical completeness</h2><p><b>Question</b></p><p>What is "metalogical completeness"?</p><p><b>Answer</b></p><p>Standard
first-order logic or predicate calculus in textbooks has an important
property called "logical completeness," which means that it can prove
all logical statements that are universally true in a non-empty domain.
This was first proved by Gödel, and I have heard it said that he was
more pleased that proof than he was with his incompleteness proofs
(although the latter of course had a much more profound impact).</p><p>In
the context of Metamath, you will also see the term "metalogical
completeness" brought up from time to time. I will try to explain what
this term means with a simple example.</p><p>Consider the set.mm theorem <a class="url outside" href="http://us.metamath.org/mpegif/a9e.html">a9e</a>, E. x x = y with no proviso (no $d) on x and y.</p><p>In
textbook predicate calculus with equality, this theorem cannot be
proved directly from the axioms. In fact, is not even a theorem but
rather a theorem scheme or "metatheorem," where x and y range over the
variables of the logic. It is important to keep in mind that x and y
are <em>not</em> actual variables of the logic, but are metavariables
ranging over them. If we call the variables of the actual logic x1, x2,
x3,… (often shown in a different typeface), then we can think of this
metatheorem as generating two kinds of infinite sets of actual
theorems. The first set is all theorems of this form where x and y are
distinct: E. x1 x1 = x2, E. x3 x3 = x2, E. x2 x2 = x1, etc. The second
set is all theorems of this form where x and y are the same: E. x1 x1 =
x1, E. x2 x2 = x2, etc. Note that in the actual theorems of the logic,
all variables are distinct by definition; x1 and x2 are the names of
two distinguished objects in an infinite set of actual variables that
we are assumed to have available to work with.</p><p>In textbook
predicate calculus, we would prove the theorem scheme E. x x = y as
follows. First, we would prove the theorem scheme x = x (for any
variable x1, x2,…), then using the theorem scheme phi -> E. x phi we
would conclude the theorem scheme E. x x = x.</p><p>Next, we start with
the scheme y = y, then use specialization to conclude the scheme E. x x
= y, where the bound variable x is a variable that is not free in the
starting expression y = y. Thus x and y must be distinct variables, i.e
if we instantiate x to be the variable x2, then we must select some
different variable such as x1 or x3 for y.</p><p>Now we must take a
metalogical leap and combine these two cases. There is no explicit rule
of predicate calculus that allows us to do this - it is more like you
use common sense, or high-level mathematical reasoning (i.e.
metamathematics), to combine them. After combining them, we conclude
the proviso-free theorem scheme (metatheorem) E. x x = y.</p><p>When I
say "common sense" above, what I really mean is the informal set theory
that is implicitly used for metamathematical reasoning. This set theory
is applied to the syntactical model of predicate calculus, which
consists of an infinite set of variables to choose from, infinite sets
of axioms generated by templates called axiom schemes (which typically
begin, "all wffs of the form…"), etc. In effect we are forming the
union of two infinite sets of theorems, then claiming (with an implicit
or explicit proof) that the template E. x x = y, with no provisos,
generates the same set of theorems as that union.</p><p>Thus, to derive
a theorem scheme of predicate calculus from other theorem schemes, we
are not directly using predicate calculus at all, in the sense of a
formal predicate calculus proof. Instead, we are really using set
theory to manipulate the schemes at a much higher level. In fact if we
used only the actual formal predicate calculus we would be severely
constrained in the kinds of general things we could say. We could not
even prove such a simple thing as "A. x phi -> phi for all wffs phi"
because that is not a theorem of predicate calculus, but a theorem
scheme.</p><p>Therefore, textbooks rarely show actual formal predicate
calculus proofs except as simple examples to illustrate what they are
talking about. The metamathematics is what lies at the heart of being
able to do useful things (i.e. prove general schemes) in predicate
calculus, and that implicitly involves informal set theory, which in
turn requires a certain level of mathematical maturity to grasp. This
may be why, even though one would think it should be the starting point
for math, predicate calculus is usually taught at an upperclass
undergraduate level. It also explains why the predicate calculus in
textbooks can be awkward to implement (as described in informal
language by textbooks) with a computer program. The problem is even
more difficult if you want to have the computer program work with
theorem schemes and not just specific formal proofs, because the
program would have to "understand" set theory, at least on some level,
in order to prove schemes like E. x x = y with no proviso.</p><p>In
set.mm, on the other hand, we can derive the scheme E. x x = y directly
from its axioms. There is no set theory involved in the derivation,
just a mechanical substitution rule. It effectively does the
metamathematics directly, without set theory. (That's why I called it
"Metamath".)</p><p>In order to achieve this, we restrict the allowable
kinds of metatheorems to be of the form that you see in set.mm's
axioms. We formally implement provisos with the $d statement. And we
state the axioms in a specific way that allows all possible
metatheorems (of this restricted form) to be proved directly from the
axioms - this is what I call <em>metalogical completeness</em> - with
no set theory or other metamathematical notions at all, other than
what's involved in its simple substitution rule. The restricted
metalogic that set.mm uses is called <em>simple metalogic.</em> All of this is more precisely described and proved in my "Finitely Axiomatized…" paper. More discussion can be found at <a class="url" href="http://us.metamath.org/mpegif/mmset.html#axiomnote">http://us.metamath.org/mpegif/mmset.html#axiomnote</a>.</p><p>Axiom <a class="url outside" href="http://us.metamath.org/mpegif/ax-17.html">ax-17</a>
is an example of an axiom that is not needed for logical completeness
but is required for metalogical completeness. Also, it can be shown
that <a class="url outside" href="http://us.metamath.org/mpegif/ax-11.html">ax-11</a>
is not needed for logical completeness, but it is unknown whether it is
needed for metalogical completeness - a problem that has been open
since its publication in 1995.</p><p>--<a class="local" href="norm">norm</a> 22-Oct-2005</p><hr><h2>ZF vs. NBG set theory in set.mm</h2><p><b>Question</b> Why does set.mm use ZF (Zermelo-Fraenkel) set theory and not NBG (von-Neumann-Bernais-Gödel) set theory?</p><p><b>Answer</b></p><p>(Several people have asked this over the years. The following is an excerpt from an email I wrote.)</p><p>I
think the fact that E. x = V doesn't hold in ZF and A. x x e. U. V (see
Quine for why "union V" and not just "V") does (the opposite of NBG) is
something of a red herring. It makes people (several so far) think that
the only reason we need virtual class variables is because ZF can't
directly handle proper classes, and therefore they wonder why I didn't
use NBG to make things simpler.</p><p>If I were to do NBG instead of
ZF, I think I would keep the set variable / class variable dichotomy
exactly the same, including the 'cv' set variable to class variable
converter. Off-hand I can't think of any definition or syntax
construction that would be different. To make it less misleading I
would rename the tokens 'set' and 'class' to something else, perhaps
'class' and 'vclass', or even the more generic 'var' and 'term' that
could apply to both ZF and NBG, but that is a human nicety and not
technically necessary.</p><p>My use of class variables follows Quine's <em>Set Theory and Its Logic</em>
very closely (who calls them "virtual classes"), and, as in Quine, does
not really commit to a specific set theory. Quine, for example, is
completely agnostic as to the set theory used, and the exact same
virtual class theory applies to both NBG and ZF as well as his own NF
set theory.</p><p>In NBG, of course, quantified variables range over
all classes, and the axioms and theorems would be different. I
initially considered NBG, and the main reason I rejected it was because
many standard (practical) theorems would have to be quantified with "A.
x e. U. V" instead of "A. x", which I thought would be a nuisance. In
retrospect, as you can see from the numerous theorems involving
unrestricted class variables, this probably wouldn't be a major
nuisance, and all of these theorems would remain exactly the same. I'm
still happy with the ZF choice, though. For one thing it's more popular
in the literature. Another reason is that it would be relatively
straightforward to convert from ZF to NBG, with almost all proofs
beyond the axiom-related stuff essentially the same, whereas the
reverse might be difficult (if quantified class tricks were used to
shorten proofs - in ZF you sometimes have to jump through hoops since
you don't have this available) to impossible (some theorems involving
proper class existence could not be converted). In this sense ZF
provides a kind of "lowest common denominator".</p><p>It is easy to
state the NBG axioms in Metamath - it is finitely axiomatized, with 7
class existence axioms in place of the Replacement scheme of ZF. And in
principle, this is all we need. However, these can be awkward to work
with, and they also cannot directly be used to prove more general
theorem schemes (metatheorems with wff variables) in Metamath. Most
texts will use these individual axioms to prove a metatheorem, called
the Class Existence Theorem, that applies to wffs with a property
called "predicative". (In versions of NBG, this is an axiom scheme
rather than a metatheorem, used in place of the 7 individual class
existence axioms.) This property might be somewhat awkward to express
in Metamath, and I haven't thought about the best way to do it. There
might be weaker-than-Class-Existence but unrestricted theorem schemes
that could be borrowed from ZF for practical work, but I'm not sure.</p><p>--<a class="local" href="norm">norm</a> 2-Nov-05</p><h2>Finitely axiomatized</h2><p><b>Question</b></p><p>By
the way let's speak about this property. The `finitely axiomatized'
property is what you discuss in your article in Notre Dame journal of
logic. This article is hard for me to understand. According to me its
aim is to describe a logic system that could be checked by a computer
program. In fact this article is the theoretical base of the metamath
program. One of the condition for checking proofs with a program is
that the axioms are in a finite number (because computers are finite
device). However in traditional logic books the axioms are in infinite
number. Therefore it's impossible to use these descriptions to realize
a computer-aided checking. And then you had to describe a system with a
finite number of axioms before realizing metamath.</p><p>But I have a
question. Having a finite number of axioms seems very natural to human
beings. For example when Margaris describes his system, he explains
what a scheme is, then he says that there is an infinite number of
axioms and eventually he explains he will make the proof with the
scheme ( considered as a class of axioms ). In fact that's what
everybody does when he makes a proof. So why is the description of
infinitely axiomated logic so frequent when in fact it is possible to
describe finitely axiomatized logic and when the genuine way to do
logic seems to use finitely axiomatized logic.</p><p>--<a class="local" href="frl">frl</a> 9-Nov-2005</p><p><b>Answer</b></p><p>Just
as "logical completeness" is a different concept from "metalogical
completeness," "finitely axiomatized" in the Metamath sense has a
different meaning than "finitely axiomatized" in the traditional sense.</p><p>In
standard logic texts, a first-order theory is defined as finitely
axiomatized if it involves a finite number of axioms (no axiom schemes)
added to first-order logic. ZF is not finitely axiomatized because it
requires a scheme for the axiom of Replacement. NBG is finitely
axiomatized because it requires no schemes.</p><p>In both the ZF and
NBG cases, however, the traditional predicate calculus underneath is
not finitely axiomatized - it requires schemes. This means that if you
start with a blank slate, and want to start writing down all possible
proofs, under traditional predicate calculus you are faced with an
infinite number of choices for the very first proof step.</p><p>In my <a class="url outside" href="http://us2.metamath.org:8888/downloads/finiteaxiom.pdf">paper</a>
(PDF file), what I call "finite axiomatization" means that at any point
in a proof, there are only finitely many choices available for the next
step of the proof. This is achieved by treating wff metavariables as
individual variables, using a carefully devised system of axioms that
is "metalogically complete." In addition, we replace the rule of modus
ponens with a unification-based equivalent rule called "condensed
detachment" (invented by logician Carew Meredith in the 1950s) and the
rule of generalization with an analogous rule I called "condensed
generalization". With this type of system, not only does predicate
calculus becomes finitely axiomatized, but so does ZF set theory (as
well as NBG, which already is anyway). Condensed detachment does the
following: given any two schemes as its inputs, its output is the most
general scheme that can result from modus ponens applied to the inputs
(assuming modus ponens can be applied; otherwise the result is
undefined, and that application of condensed detachment would not be a
legal proof step).</p><p>The system S1 presented in my paper is finitely axiomatized. The <a class="url outside" href="http://us.metamath.org/mmsolitaire/mms.html">Metamath Solitaire</a>
applet is internally an implementation of system S1 (actually a mild
extension incorporating distinct variables), although to the user the
results are displayed in the notation of "regular" Metamath (system S3'
in the paper). The finite axiomatization means that there are only
finitely many choices for the next step in any proof. To create the
proof, the user successively selects from the possible choices
displayed in a dynamic drop-down list. It would be impossible to make a
Metamath Solitaire for the traditional axiomatization of predicate
calculus, because the drop-down list would be infinitely long.</p><p>The
fact that all possible proofs can be simply enumerated in system S1
means it is easy to write a computer program to exaustively search for
shortest proofs (although the exponential search space blows up at
around 20 steps, and beyond that heuristic techniques are needed).
Early on I did this for propositional calculus for fun, and later it
was done in a much more sophisticated way by Greg Bush. The results can
be seen on the "<a class="url outside" href="http://us.metamath.org/mmsolitaire/pmproofs.txt">shortest
known proofs</a>" page for the propositional calculus theorems in <em>Principia Mathematica</em>.</p><p>Now,
the fact that traditional predicate calculus is not finitely
axiomatized doesn't mean that computers can't "do" math with it, of
course. There are algorithms, such as Robinson's resolution, that,
given a <em>specific</em> theorem of predicate calculus (not a theorem
scheme), will attempt to find a proof. This proof can, in principle, be
translated back into a sequence of axiom and rule references to
instances of the traditional schemes of the Hilbert-style formulation.</p><p>Resolution-type
theorem provers have, in effect, the schemes of predicate calculus
embodied in their algorithms. However, beyond that, they cannot deal
with schemes - i.e. schemes outside of the implicit schemes their
algorithms embody. So, if they are to be used to prove theorems in a
first-order theory added to predicate calculus, that theory must
contain a finite number of axioms.</p><p>In particular, resolution-type
theorem provers cannot, in general, prove theorems of ZF set theory. On
the other hand, they can work with NBG set theory by using its finitely
axiomatized version. This is why set theory work with the Otter theorem
prover has focused on NBG.</p><p>There is a twist to all of this: a resolution-type theorem prover <em>can</em>
encode the finitely axiomatized Metamath-type system S1, along with any
first-order extension such as ZF set theory. We define the predicate
"is provable", then give the resolution prover the <em>denial</em> of
the sentence "theorem xxx is provable in system S1". The prover will
then go off and try to prove us wrong by refuting this denial - and
when it succeeds, its refutation will encode a Metamath proof! So, in
this very indirect way, resolution-type theorem provers <em>can</em>
do ZF set theory! The problem is that there is eventually an
exponential blow-up of possibilities it must explore to find a
refutation, and practically speaking this technique will only work for
theorems that have short proofs (although hints - theorems already
proved - could be given to guide it along). I played around with this
with the Otter theorem prover in the early 90's on a 40MHz Macintosh
IIfx with 8MB memory, showing that this works in principle - it found
Metamath proofs, and even found a clever, shorter proof for the theorem
scheme "A. x P -> A. x A. x P" than the one I found by hand (this was
the D4GD4G5 proof in the Appendix of my <a class="url outside" href="http://us2.metamath.org:8888/downloads/finiteaxiom.pdf">paper</a>) - but it ran out of steam at about ten proof steps or maybe less, as I vaguely recall.</p><p>--<a class="local" href="norm">norm</a> 9-Nov-2005</p><h2>Conjecture</h2><p>|- ( [ x / y ] y = z &lt;-> x = z ) provided that <img class="InlineMath" src="metamathMathQuestions_files/gBTdU400FlVEVx3YL0JBQ.png" alt="$ d y z  $">.</p><p>I can't find this proof in metamath. Is it wrong, is it useless ?</p><p>--<a class="local" href="frl">frl</a> 3-Dec-2005</p><p>It's correct. Here's a proof in Ghilbert syntax:</p><pre>  import (SET_MM_AX zfc/set_mm_ax () "")
  import (SET_MM zfc/set_mm (SET_MM_AX) "")</pre><pre>  var (set x y z w)</pre><pre>  thm (frl051203lem ((x y) (y z)) ()
    (&lt;-&gt; ([/] (cv x) y (= (cv y) (cv z))) (= (cv x) (cv z)))
    (
     x y equsb2
        x y z equequ1
        x y sbimi
     ax-mp
     x y (= (cv x) (cv z)) (= (cv y) (cv z)) sbbi mpbi
       (= (cv x) (cv z)) y ax-17 x sbf
     bitr3
  ))</pre><pre>  thm (frl051203 ((y z)) ()
    (&lt;-&gt; ([/] (cv x) y (= (cv y) (cv z))) (= (cv x) (cv z)))
    (
     (= (cv y) (cv z)) w ax-17 x y sbco2
       w y z frl051203lem
       x w bisb
         x w z frl051203lem
       bitr
     bitr3
  ))</pre><p>The
lemma proves the theorem with the additional dv constraint x,y, and the
main theorem gets rid of that dv constraint by introducing a new
variable (w), and making that take the place of y with respect to the
constraint.</p><p>As to whether it's useful, that mostly depends on whether you've got other theorems that make use of it :)</p><p>– <a class="local" href="raph">raph</a> 3-Dec-2005</p><p>Thank
you for this beautiful proof Raph. Concerning the fact something is
useful or not I often notice that what I consider mandatory is viewed
as absolutely optional by Norm :) I succeeded in repeating frl051203lem
but (for me) the order of the steps is not yet as clear for frl051203.</p><p>– <a class="local" href="frl">frl</a> 4-Dec-2005</p><p>I added Raph's proofs to set.mm. The proofs above are in <a class="url outside" href="http://us2.metamath.org:8888/mpegif/equsb3lem.html">equsb3lem</a> and <a class="url outside" href="http://us2.metamath.org:8888/mpegif/equsb3.html">equsb3</a>.</p><p>Another example of this technique is shown by <a class="url outside" href="http://us2.metamath.org:8888/mpegif/sbccomg.html">sbccomg</a>, which takes the otherwise identical <a class="url outside" href="http://us2.metamath.org:8888/mpegif/sbccomglem.html">sbccomglem</a>
and eliminates 4 of its 7 distinct variable pairs. In this case, the
separate lemma reduces the database size significantly, since it is
referenced four times in the main theorem.</p><p>By the way in the past couple of weeks I have introduced proper substitution into class variables into set.mm: <a class="url outside" href="http://us2.metamath.org:8888/mpegif/df-csb.html">df-csb</a>. By clicking on "related theorems" you can see a bunch of theorems for it.</p><p>Also
in the past couple of weeks, I have renamed a large number of theorems
with "bi" in their names, e.g. bisb (which is used above) became sbbii,
to be consistent with the "eq" series such as dmeq, dmeqi, dmeqd. I
have been doing this over time, and I hope with this last change I have
finally completed it. The changes are all documented at the beginning
of set.mm</p><p>– <a class="local" href="norm">norm</a> 4-Dec-2005</p><h2>Axiom of variable substitution: ax-11</h2><p><b>Question</b></p><p>According
to its name, this axiom is connected to the [ x / y ] operator. But
what connexion exactly ? Could we reformulate this axiom using the
substitution operator ?</p><p>– <a class="local" href="frl">frl</a> 13-Dec-2005</p><p><b>Answer</b></p><p>Recall <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ax-11.html">ax-11</a>:</p><pre>  ( -. A. x x = y -&gt; ( x = y -&gt; ( ph -&gt; A. x ( x = y -&gt; ph ) ) ) )</pre><p>Theorem <a class="url outside" href="http://us2.metamath.org:8888/mpegif/sb6.html">sb6</a>
shows that the wff "A. x ( x = y -> ph )" is equivalent to "[ y / x ]
ph" when x and y are distinct, i.e. when "-. A. x x = y". This is the
connection to substitution.</p><p>On the other hand, if we blindly
replace "A. x ( x = y -> ph )" with "[ y / x ] ph" in ax-11, the
"essence" of the axiom vanishes, and we merely get a weakening of <a class="url outside" href="http://us2.metamath.org:8888/mpegif/sbequ1.html">sbequ1</a>:</p><pre>    ( x = y -&gt; ( ph -&gt; [ y / x ] ph ) )</pre><p>which is derivable without invoking ax-11. So this would not give you the reformulation you seek.</p><p>Still,
it should be possible to state an axiom containing the substitution
operator that could replace ax-11, but I'm not sure what form it would
take. It might be interesting to try to work that out. A possible
candidate is <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dfsb2.html">dfsb2</a>, since I wasn't able to derive it without invoking ax-11, but that is just a guess. Theorem <a class="url outside" href="http://us2.metamath.org:8888/mpegif/sb4.html">sb4</a> is another promising candidate.</p><p>Overall,
ax-11 is a rather mysterious and subtle axiom. It is not needed for
logical completeness (which means that any theorem without wff
variables can be proved without ax-11, as is shown in my "Finitely
Axiomatized…" paper). But so far no one has proved or disproved that it
is needed for metalogical completeness. Off and on I have made
unsuccessful attempts to prove it from the others. The furthest I got,
after considerable effort, was to prove the $d elimination theorem
without using ax-11: <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dvelimf2.html">dvelimf2</a>. This result was something of a surprise to me and may be useful for further work on the problem.</p><p>--<a class="local" href="norm">norm</a> 15-Dec-05</p><p>Thank you Norm for this clearing up. I better understand this axiom and its  mystery.</p><p>--<a class="local" href="frl">frl</a> 16-Dec-05</p><p><b>BIG NEWS!!</b> <a class="local" href="jarpiain">Juha Arpiainen</a> has proved the independence of axiom ax-11. See the Metamath <a class="url outside" href="http://us2.metamath.org:8888/mpegif/mmrecent.html#new">Most Recent Proofs</a> page for the details and very clever proof of this 11-year-old open problem.</p><p>--<a class="local" href="norm">norm</a> 21-Jan-2006</p><p>Guys your are really, very, very good !</p><p>--<a class="local" href="frl">frl</a> 21-Jan-06</p><h2>$d statement, distinctor and ax-16</h2><p><b>Question</b></p><p>For
a while I had thought that the `$d statement is absolutely unnecessary
in principle' (I'm auto-quoting myself from a mail to Norm that he was
kind enough to quote in a page of the metamath site). But it seems to
me now that it is more difficult than that.</p><p>Individual variables are variables that can be replaced by other individual variables and by nothing else. So `<img class="InlineMath" src="metamathMathQuestions_files/yTwblCuMsBw2pgfqo3J77A.png" alt="$ d x y  $">.' means `x' and `y' can't be replaced by the same variable.</p><p>Norman Megill calls `-. A. x = y' a distinctor because it means that x and y have to be distinct.</p><p>ax-16 says that ` ( A. x x = y -> ( ph -> A. x ph ) )' provided that x and y are distinct (`$d x y').</p><p>If
a distinctor and a $d statement were synonyms, we could replace the
`$d' statement by a distinctor in ax-16. In this case we would have:</p><pre>   ax-16.1 $e -. A. x = y $.
   ax-16 ( A. x x = y -&gt; ( ph -&gt; A. x ph ) )</pre><p>But obviously Norm didn't do that, and in a certain way he couldn't do that, but for me it is not yet clear why.</p><p>– <a class="local" href="frl">frl</a> 4-Jan-2006</p><p><b>Answer</b></p><p>In general, $d x y can be replaced with an <em>antecedent</em>
consisting of the "distinctor" -. A. x x = y, and vice-versa. With
various theorem-proving techniques summarized under the heading
"Guidelines for adding and removing $d's in proofs in set.mm" above,
the distinctor version of a theorem can be converted to the $d version
and vice-versa.</p><p>If you replace "$d x y" with the <em>hypothesis</em> "-. A. x x = y" (as you did in your <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ax-16.html">ax-16</a>
example), this is also a sound thing to do; your modification of ax-16
is still a theorem. But I would conjecture that in general this weakens
a theorem too much to recover its $d version with predicate calculus
only (short of reproving the $d version from scratch). In particular,
the Weak Deduction Theorem cannot be applied, since there is no
substitution instance that makes the hypothesis true under predicate
calculus. However, you can still recover the $d version in set theory,
using the theorem <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dtru.html">dtru</a> to eliminate the hypothesis.</p><p>But even replacing the $d in ax-16 with the <em>antecedent</em>
-. A. x x = y gives us ( -. A. x x = y -> ( A. x x = y -> ( ph -> A. x ph
) ) ) which is true by propositional calculus. So this is a sound thing
to do. But it would be kind of pointless, since it "cancels" the
strength of ax-16, whose purpose is to specify the property of the $d
in the first place.</p><p>Finally, if we omit ax-16 entirely, using
distinctor antecedents to serve the same purpose, then according to
Andréka's theorem mentioned above there will be some theorems that will
not be provable without redundant distinctors. For example, instead of
being able to prove ( x = y -> y = x ), we might only be able to prove (
-. A. z z = x -> ( x = y -> y = x ) ). (This is not a real example, since
we <em>can</em> prove the first one. I've been unable to find a real example, but Andréka's theorem says they exist.)</p><p>--<a class="local" href="norm">norm</a> 6-Jan-2006</p><p>From
my pseudo-example of Andréka's theorem, ( -. A. z z = x -> ( x = y -> y =
x ) ), I can now see how to recover ( x = y -> y = x ) without proving
it directly: we substitute y for z and also prove separately ( A. y y =
x -> ( x = y -> y = x ) ), then apply <a class="url outside" href="http://us2.metamath.org:8888/mpegif/pm2.61i.html">pm2.61i</a>.
I think this technique would apply to any theorem of the form ( -. A. z
z = x -> &lt;wff&gt; ) where z is a variable not in &lt;wff&gt;: we
substitute a variable y in &lt;wff&gt; for z and use <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ax-10.html">ax-10</a>
in particular to help us obtain the ( A. y y = x -> &lt;wff&gt; )
version. This is essentially the Distinctor Reduction Theorem in my <em>Finitely Axiomatized…</em> paper.</p><p>Therefore,
I would conjecture that any example of Andréka's theorem must have at
least two distinctor antecedents: ( -. A. z z = x -> ( -. A. z z = y ->
&lt;wff&gt; ) ), where &lt;wff&gt; has at least two variables x and y
and where z is a variable not in &lt;wff&gt;.</p><p>In case you are
wondering, the distinctor antecedents with z would arise from a dummy
variable z that would occur in the $d version of the proof of
&lt;wff&gt;. Andréka's theorem says that some theorems are impossible
to prove without invoking such dummy variables, and moreover there is
no upper bound on how many such dummy variables might be needed. In a
system of predicate calculus that omits ax-16, we would introduce these
dummy variables into a proof by using distinctors. Subject to this
constraint - that per Andréka's theorem we will accumulate distinctor
antecedents that can't be eliminated - my paper proves that the system
ax-1 through ax-15 + ax-mp + ax-gen is otherwise a complete system of
predicate calculus with equality and a single binary predicate.</p><p>--<a class="local" href="norm">norm</a> 7-Jan-2006</p><h2>Size of a proof</h2><p>Hi Norm, in the news you often say that a new proof is shorter but how do you calculate it ?</p><p>--<a class="local" href="frl">frl</a> 27-Feb-2006</p><p>From set.mm:</p><pre>  Usually I will automatically accept shorter proofs that (1) shorten the
  set.mm file (with compressed proofs), (2) reduce the size of the HTML
  file generated with SHOW STATEMENT xxx / HTML, (3) use only existing,
  unmodified theorems in the database (the order of theorems may be
  changed though), (4) use no additional axioms, and (5) involve none of
  the special cases listed below [see set.mm for special cases].</pre><p>--<a class="local" href="ocat">ocat</a> 27-Feb-2006, revised <a class="local" href="norm">norm</a> 15-Mar-2006</p><p>Oh
yes thanks and is there a way to count the steps of a proof where all
the intermediate rules would have been replaced by their own proofs
recursively until only the axioms remain ? – <a class="local" href="frl">frl</a> 28-Feb-2006</p><p>Yes, the 'trace_back' command will do this, up to a billion steps.</p><pre>  MM&gt; show trace_back prth / count_steps / essential
  The statement's actual proof has 5 steps.  Backtracking, a total
  of 56 different subtheorems are used.  The statement and
  subtheorems have a total of 199 actual steps.  If subtheorems
  used only once were eliminated, there would be a total of 25
  subtheorems, and the statement and subtheorems would have a total
  of 143 steps.  The proof would have 2589 steps if fully expanded.
  The maximum path length is 22.  A longest path is:  prth &lt;- imp4b
  &lt;- imp4a &lt;- impexp &lt;- imbi1i &lt;- impbi &lt;- bi3 &lt;- expi &lt;- expt &lt;-
  pm3.2im &lt;- con2d &lt;- con2 &lt;- nega &lt;- pm2.18 &lt;- pm2.43i &lt;- pm2.43
  &lt;- pm2.27 &lt;- id &lt;- mpi &lt;- com12 &lt;- syl &lt;- a1i &lt;- a1i.1 .</pre><p>Beyond
2^31 steps, the arithmetic overflows the way the program is written
now. Rather than mislead the user with a garbage number, it stops at a
billion and just prints "&gt;1000000000".</p><pre>  MM&gt; show trace_back 2p2e4 / count_steps / essential
  The statement's actual proof has 11 steps.  Backtracking, a total
  of 2153 different subtheorems are used.  The statement and
  subtheorems have a total of 22775 actual steps.  If subtheorems
  used only once were eliminated, there would be a total of 1569
  subtheorems, and the statement and subtheorems would have a total
  of 21535 steps.  The proof would have &gt;1000000000 steps if fully
  expanded.  The maximum path length is 134.  A longest path is:
  2p2e4 &lt;- 2cn &lt;- 2re &lt;- readdcl &lt;- axaddrcl &lt;- addresr &lt;- 0idsr &lt;-
  [...]</pre><p>Note
that 'show trace_back' without '/count_steps' will list all subtheorems
found during backtracking. In principal, this backtrack list could be
used (by a program or metamath function that does not yet exist) to
create a minimal set.mm subset needed to prove a particular theorem.
Without '/essential', the steps used to construct wffs (that you see in
'show proof xxx/all') will also be counted. 'show
trace_back/essential/axioms' tells you the axioms and definitions
assumed for the proof.</p><p>--<a class="local" href="norm">norm</a> 28-Feb-2006</p><p>Well
thank you. I wonder if it is possible to prove that a given proof is
the shortest one (once all the steps have been recursively replaced by
the axioms). --<a class="local" href="frl">frl</a> 1-Mar-2006</p><p>The
only way I know of to "prove that a given proof is the shortest one"
from the axioms is exhaustive enumeration. The only project (that I am
aware of) that has been concerned with this is the <a class="url outside" href="http://us2.metamath.org:8888/mmsolitaire/pmproofs.txt">"Shortest known
proofs of the propositional calculus theorems from ''Principia
Mathematica''"</a>.
All proofs up to length 21 (or maybe 23) were found by exhaustive
enumeration of all possible proofs, so they are therefore the
theoretical shortest. Any longer proofs were found by "cleverness" and
may not be the shortest possible. --<a class="local" href="norm">norm</a> 2-Mar-2006</p><h2>The Bernays problem</h2><p><b>Question</b></p><p>Here
is the problem. A (propositional calculus) theorem (let's say stricto
sensu; not an inference) being given, is it possible to design a system
of axioms from which the theorem can't be derived? I think we can name
this problem (if it is a problem after all) after the creator of
intuitionism. I have always found highly funny the decision that
Brouwer had taken to remove a certain theorem from propositional
calculus (namely `the tertium non datur') only because he considered it
to be immoral. --<a class="local" href="frl">frl</a> 6-Mar-2006</p><p><b>Answer</b></p><p>I asked mathematician Eric Schechter to answer this. He has written a wonderful book, <a class="url outside" href="http://www.math.vanderbilt.edu/%7Eschectex/logics/">Classical and Nonclassical Logics:
an introduction to the mathematics of propositions</a>,
that covers these kinds of topics and that I highly recommend. This is
not just another introductory logic book, but it goes into many exotic
propositional logics that are topics of recent research, yet it
provides all the necessary background in a very clear and readable
style, with few mathematical prerequisites. </p><dl class="quote"><dt></dt><dd>Pretty
expensive ( as a French I'm always amazed by the price of technical
books in the anglo-saxon world ) but I'll ask main public libraries in
Paris whether they can add it on their shelves. – <a class="local" href="frl">frl</a> 9-Mar-2006<dl class="quote"><dt></dt><dd>I'm happy to announce that <a class="url outside" href="http://www.math.vanderbilt.edu/%7Eschectex/logics/">Classical and Nonclassical Logics:
an introduction to the mathematics of propositions</a>
is now available in free access in Paris at the library of Beaubourg.
The book is new; it is very well printed; it is very well written; the
pedagogical sense of Schechter is perfect; his sense of humour is great
( I have really liked his development about relationships between
insomnia and set theory ) and his way of presenting logics is different
from the tradional books with an emphasis put on the non classical
logics. His presentation of semantic is crystal clear. The only regret
is that there is no section about predicate calculus. <a class="local" href="frl">frl</a> 1-Jul-2006</dd></dl></dd><dt></dt><dd>.</dd><dt></dt><dd>.</dd><dt></dt><dd>University
libraries may have it already, or if not would probably be receptive to
adding it. The MIT library has it, for example. – <a class="local" href="norm">norm</a> 10-Mar-2006</dd></dl><p>Here
is his response (the period between paragraphs is mine because I don't
know how to put a blank line between indented paragraphs - anyone?):</p><dl class="quote"><dt></dt><dd>The question doesn't make much sense unless we add the requirement that the system of axioms <b>can</b>
derive certain other things. In other words, we want to find a system
of axioms (and assumed inference rules, though in many cases detachment
is enough) such that certain given formulas are derivable, but certain
other formulas are not derivable.</dd><dt></dt><dd>.</dd><dt></dt><dd>The
answer is yes, there are plenty of examples of that. In fact, my whole
book is chock full of them; that's just the kind of example that my
book is all about. The simplest examples are given by finite matrix
interpretations.</dd><dt></dt><dd>.</dd><dt></dt><dd>All we need is
soundness. That's the easy half of completeness; it's discussed in
Chapter 21 (a very short chapter). It is a way of establishing that all
the theorems of some axiomatic system are tautologies (i.e.,
always-true formulas) of some interpretation. It then follows
(contrapositively) that any non-tautology is not derivable from the
axioms.</dd><dt></dt><dd>.</dd><dt></dt><dd>(Of course, the fact that we can use this contrapositive is because I'm assuming the <b>metalogic</b>
is classical. If we use some nonclassical logic for our metalogic, we
could run into complications, since some versions of the contrapositive
law fail in some nonclassical logics.)</dd><dt></dt><dd>.</dd><dt></dt><dd>For
instance, tertium non datur (the law of excluded middle) is not
derivable from the axioms of intuitionistic logic, which I prefer to
call constructive logic. To see that, we just need to consider some
topological interpretations. All the axioms, and therefore all the
theorems, of constructive logic hold in every topological
interpretation; that's established in 22.14 of my book. But tertium non
datur fails in many (though not all) topological interpretations. For
instance, it fails for the Euclidean topology of the real line (see
10.4). It also fails for a finite interpretation, given by taking the
set {0,1} and equipping it with the topology { emptyset, {0}, {0,1} }.</dd><dt></dt><dd>.</dd><dt></dt><dd>But perhaps <a class="local" href="frl">frl</a>
actually wanted not just some examples, but an algorithm for producing
examples to fit some particular specifications. That's a much harder
problem, one for which I don't have an answer. And, depending on how
the problem is posed, it might even be unsolvable.</dd><dt></dt><dd>.</dd><dt></dt><dd>I do have a partial answer: There are some computer programs that can <b>assist</b> with looking for such examples. Some of the best programs of this type are described at<dl class="quote"><dt></dt><dd><a class="url" href="http://rsise.anu.edu.au/csl/index.php?module=ContentExpress&amp;func=display&amp;ceid=8&amp;meid=-1">http://rsise.anu.edu.au/csl/index.php?module=ContentExpress&amp;func=display&amp;ceid=8&amp;meid=-1</a></dd></dl></dd><dt></dt><dd>Among those, the only one I've used is<dl class="quote"><dt></dt><dd>MaGIC<a class="edit" title="Click to edit this page" href="http://planetx.cc.vt.edu/AsteroidMeta?action=edit;id=MaGIC">?</a> (Matrix Generator for Implication Connectives)</dd></dl></dd><dt></dt><dd>which
was written by John Slaney and ported to Windows by Norm Megill. And
I've really only used it once. I used it to discover the interpretation
that I call "Church decontractioned", which is described in sections
9.14-9.18 of my book. I found it as an example to show that what I call
"basic logic" (chapters 13-14), plus the formula</dd></dl><pre>      specialized contraction (15.3.a)      (A -&gt; (A -&gt; ~A)) -&gt; (A -&gt; ~A)</pre><dl class="quote"><dt></dt><dd>is not sufficient for deriving</dd></dl><pre>      weak contraction (15.2.b)              (A -&gt; (A -&gt; ~B)) -&gt; (A -&gt; ~B)</pre><dl class="quote"><dt></dt><dd>–
in other words, specialized contraction is strictly weaker than weak
contraction, even in the presence of all of basic logic. To prove that
fact, we simply observe that "Church decontractioned" is sound for
basic logic plus specialized contraction, but weak contraction is not
tautological in that interpretation.</dd><dt></dt><dd>.</dd><dt></dt><dd><em>…name this problem (if it is a problem after all) after the creator of intuitionism.</em></dd><dt></dt><dd>.</dd><dt></dt><dd>I
don't see any reason to name it after Brouwer. If you want to name it
after someone, maybe a better choice would be Paul Bernays. Rescher's
book "Many-valued Logic" says that Bernays was the first to use
multivalued logics to demonstrate that some formula cannot be derived
from some axiomatic system.</dd></dl><p>Thank you, Eric.</p><p><a class="local" href="norm">norm</a> 7-Mar-2006</p><p>Thank you Eric, thank you Norm <a class="local" href="frl">frl</a></p><h2>Compressed Sub-Proof Start Demarcation</h2><p><b>Question</b></p><p>Metamath.pdf's description of sub-proof in the documentation of Compressed Proofs is not explicit about the <b>start</b>
of a sub-proof and how to find it. I am thinking that the "Z" marks the
end of a sub-proof and that the number immediately prior to a "Z" will
correspond to one of the parenthesized labels?</p><p>Also, the parenthesized labels are the non-syntax assertions referenced in the proof?</p><p>Excerpt from Metamath.pdf:</p><pre>    The letter Z identifies (tags) a proof step that
    is identical to one that occurs later on in the
    proof; it helps shorten the proof by not
    requiring that identical proof steps be proved
    over and over again (which happens often when
    building wff’s). The Z is placed immediately
    after the least-significant digit (letters A
    through T) that ends the integer corresponding to
    the step to later be referenced.</pre><pre>    The integers that the upper-case letters
    correspond to are mapped to labels as follows. If
    the statement being proved has m mandatory
    hypotheses, integers 1 through m correspond to
    the labels of these hypotheses in the order shown
    by the show statement ... / full command, i.e.,
    the RPN order of the mandatory hypotheses.
    Integers m+ 1 through m+ n correspond to the
    labels enclosed in the parentheses of the
    compressed proof, in the order that they appear,
    where n is the number of those labels. Integers m
    + n + 1 on up don’t directly correspond to
    statement labels but point to proof steps
    identified with the letter Z, so that these proof
    steps can be referenced later in the proof.
    Integer m+n+1 corresponds to the first step
    tagged with a Z, m+n+2 to the second step tagged
    with a Z, etc. When the compressed proof is
    converted to a normal proof, the entire subproof
    of a step tagged with Z replaces the reference to
    that step.</pre><p>(I'm having one of my El Stupido days. Eventually I could probably figure out the algorithm but I am hoping for a big clue.)</p><p>Thanks. <a class="local" href="ocat">ocat</a> 14-Mar-2006</p><p><b>Answer</b></p><p>OK, here's a first attempt at an explanation. We'll keep trying if it doesn't suffice.</p><p>Consider
first a compressed proof with no Z's in it. Hopefully you are at the
point where you can see how this can be translated to a standard RPN
proof.</p><p>Now, given anything in RPN - whether a proof or a formula
- if you pick an arbitrary point in the middle, there is a smallest
sequence of previous steps that leads to that point. Think of an
arithmetical expression in RPN: (2+3)+4=9 in RPN is 23+4+. For any
point in that expression, there is a corresponding subformula. Going
left to right, the subformulas are: 2, 3, 23+, 4, 23+4+.</p><p>When it
is verifying a proof, the Metamath program actually doesn't care about
subproofs. It scans along the the compressed proof string, pushing and
popping the stack as it goes along. Whenever it sees a Z, it simply
saves the stack top for later use whenever it gets referenced later in
the proof. It is that simple, and it makes verification fast.</p><p>For
some purposes, such as translating the proof to non-compressed, you do
need to know where subproofs start. To do this, you scan the proof from
left to right and populate a parallel integer array with the length of
the subproof at each step. For the formula example above, the array for
23+4+ above would have subexpression lengths 1,1,3,1,5. (A number has
subproof length 1. When a + is encountered, you add the lengths of the
two subexpressions it connects, plus 1. So to compute all subexpression
lengths, you need only a one-pass scan through the RPN expression.)
Then whenever a subproof is referenced, you copy as many steps as are
indicated by this parallel array. A subproof can reference another
subproof, but if you transform the compressed proof into the complete
non-compressed proof as you scan along, copying the corresponding piece
of the "subproof length" array along with the subproof, this should all
take care of itself automatically, in a single pass through the proof.</p><p><em>Also, the parenthesized labels are the non-syntax assertions referenced in the proof?</em></p><p>The
label list includes any non-mandatory $f's (used for dummy variables in
the proof) as well as all the assertions used by the proof, syntax and
non-syntax, in any order. Its purpose is to map long label names to the
short uppercase "numbers" in the compressed proof. The label list
doesn't include mandatory $f's nor $e's, since they can be inferred
from the "frame" (context) of the $p statement and thus would be
redundant. This of course has an impact on how statements with
compressed proofs can be edited, which is discussed in Section 2.5 of
the book.</p><p>--<a class="local" href="norm">norm</a> 14-Mar-2006</p><p>(I copied the following from the <a class="local" href="mmj2Feedback">mmj2Feedback</a> page since it supplements this topic.)</p><p>In <a class="local" href="marnix">marnix</a>'s 440-line <a class="local" href="Hmm">Hmm</a> proof verifier, the compressed proof code (located in <a class="url outside" href="http://www.solcon.nl/mklooster/repos/hmm/HmmImpl.hs">HmmImpl.hs</a>) is 80 lines long. The compressed proof format is precisely documented in Appendix B of the <a class="url outside" href="http://us2.metamath.org:8888/downloads/metamath.pdf">Metamath book</a>,
from which Marnix wrote his verifier. He provided me with this helpful
outline of his nice algorithm, which I have now incorporated into the
Metamath program also:</p><pre>  A..T stand for 1..20 in base 20, and U..Y stand for 1..5 in base 5. (So
  not 0..19 and 0..4.)  This is when counting A=1, etc., as in the book;
  which seems most natural for this encoding.  Then decoding can be done
  as follows:</pre><pre>   * n := 0
   * for each character c:
      * if c in ['U'..'Y']: n := n * 5 + (c - 'U' + 1)
      * if c in ['A'..'T']: n := n * 20 + (c - 'A' + 1)</pre><pre>  For encoding we'd get (starting with n, counting from 1):</pre><pre>   * start with the empty string
   * prepend (n-1) mod 20 + 1 as character using 1-&gt;'A' .. 20-&gt;'T'
   * n := (n-1) div 20
   * while n &gt; 0:
      * prepend (n-1) mod 5 + 1 as character using 1-&gt;'U' .. 5-&gt;'Y'
      * n := (n-1) div 5</pre><p>--<a class="local" href="norm">norm</a> 15-Mar-2006</p><h2>Proof Compression Algorithm</h2><p><b>Question</b></p><p>Hi
Norm, I have coded Decompression (though not yet tested it), and I have
come to the conclusion that I must code Proof Compression also. I
imagine that seeing a 600 line uncompressed proof pop up in the mmj2
Proof Assistant would be most unwelcome. Given that there are only 2
mmj2 "customers", I must go the extra mile and not annoy either of you
:0)</p><p>For it is written:</p><pre>    For I say unto you,
    That unto every one which hath shall be given;
    and from him that hath not,
    even that he hath shall be taken away
    from him.
    (Luke 19:26 KJ)</pre><p>The
problem of compressing a Metamath proof in the most efficient manner,
either as a "one off" or as part of a batch (all proofs in a file), is
interesting. It seems to amount to finding duplicate subtrees of depth
&gt; 1 in a proof tree and replacing them with pointers (or in
technical terms, "gnarfling the garthok".)</p><p>Do you have an elegant algorithm in hand?</p><p>Thx. <a class="local" href="ocat">ocat</a> 18-Mar-2006</p><p><b>Answer</b></p><p>Well, I don't know if it's "elegant", but the functions are relatively short.</p><p>The encoding algorithm used by the metamath program involves 2 functions: </p><dl class="quote"><dt></dt><dd>1. 'nmbrSquishProof' creates a "compact" proof.</dd><dt></dt><dd>2. 'compressProof' generates the compressed proof syntax.</dd></dl><p>These two functions exist in mmdata.c.</p><p>A
"compact" proof is a superset of the normal RPN proof syntax that has
backreferencing internal labels. You can see it if you use the
(undocumented) 'save proof xx/compact' command then look at the new
source output.</p><p>The 'nmbrSquishProof' algorithm works
approximately like this (looking at the comments in that function,
which I haven't visited in years). It scans forward through the proof
and considers the subproof at the current step. It scans the rest of
the proof from the current step, and anywhere it finds that subproof,
it replaces it with a reference to the current step. The reference
consists a negative number representing the offset of the earlier
subproof; negative is used so it won't be confused with a real label,
which is always a positive integer. (Internally it doesn't store the
actual labels, but their "statement number", so the proof is an array
of integers.)</p><p>Because we always want the longest possible earlier
subproofs to be referenced, the replacements are done in a parallel
array so that the subproof matches won't be upset by earlier
replacements. Or something like that…I refer the reader to mmdata.c.
Hey, it works. :) It is kind of fuzzy to me now, and I don't have a lot
of time to spend at the moment, but ask me more questions if you need
help figuring it out and I will look at it more deeply.</p><p>(There
are probably other ways to do this, but the idea is just to find the
longest match to an earlier subproof. I suspect you'll probably come up
with you own way of doing it that may even be better.)</p><p>The
'compressProof' function is just a mechanical translation to the
compressed proof syntax. It will actually work with an "unsquished"
(standard RPN) proof, which is a subset of the "compact" proof syntax,
although it wouldn't make sense to do that; my point is that it is
completely independent of 'nmbrSquishProof'.</p><p>--<a class="local" href="norm">norm</a> 18-Mar-2006</p><dl class="quote"><dt></dt><dd>Thanks.
I will review your algorithms and make a serious attempt at this
problem. I spent some time on Google looking for related information.
Came across Dick Grune's name again, and found that people all over the
world are beavering away on "repeated subproofs" and tree compression.
I will post some links that seem interesting (though not obviously
useful on this problem). --<a class="local" href="ocat">ocat</a> 18-Mar-2006</dd></dl><h2>Data Mining set.mm</h2><p><b>Question</b></p><p>I
wonder how many repeated subproofs are repeated across theorems in
set.mm – not including "syntax proofs". And might there be repeats that
merely use different variables and could be unified to a common
subproof? Perhaps we could data mine set.mm's proofs to discover
candidates for theorem-hood? --<a class="local" href="ocat">ocat</a> 24-Mar-2006</p><p><b>Answer</b></p><p>I
am always on the lookout for common subproofs, and I will add one as a
new theorem when the total size of the new theorem (including its
comment) is less than the total size it trims off of the proofs that it
ends up shortening. In other words, I always like to add a new theorem
that "pays" for itself by reducing the total size of set.mm (measured
with compressed proofs).</p><p>Indeed, I encourage people to try to discover such patterns, and that is one way to get your name into set.mm. From set.mm,</p><dl class="quote"><dt></dt><dd>Usually I will also automatically accept a <em>new</em>
theorem that is used to shorten multiple proofs, if the net size of
set.mm (including the comment of the new theorem) is decreased as a
result.</dd></dl><p>Whenever (based on the date stamp) you see a more
recent theorem used to prove an older one, most likely the more recent
one was a new recurring subproof that was discovered. One example is
peano2re, which just says "( A e. RR -> ( A + 1 ) e. RR )", replacing
the sequence ax1re, mpan2, axaddrcl. This shortens 43 proofs and more
than "pays" for itself.</p><p>Something like peano2re that saves only a
step or two typically needs to be used between 10 and 20 times before
"paying" for itself.</p><p>When I notice a pattern I think I've seen a
number of times before, I'll test a new theorem for the pattern by
running a script that attempts to use it everywhere possible with the
Proof Assistant's "minimize_with" command. Too often I find I was
deluded by determining that it shortens only say 2 or 3 proofs, in
which case I will scrap it unless I think it will eventually pay for
itself in the future.</p><p>Every now and then I'll run across a gem
that will pay for itself in only one extra proof that it simplifies.
One of the most impressive examples is en2/dom2 and friends, which
simplify equinumerosity proofs so dramatically, by removing all of the
messiness of existentially quantified 1-1 functions and so on, that
their compressed proofs listings are sometimes shorter than the
textbook versions in terms of number of lines. Look at <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dom2.html">dom2</a>:
from very simple hypotheses, we conclude the relatively advanced
relation of dominance (which is indicated by all the definitions the
proof is based on, shown on the bottom of the proof page). The proof of
dom2 (actually <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dom2d.html">dom2d</a>)
takes all the messy function stuff and distills it down to something
very simple, involving no quantifiers and nothing more advanced than =
and epsilon. I'm actually rather proud of it. :)</p><p>--<a class="local" href="norm">norm</a> 26-Mar-2006</p><h2>Minimizing proofs</h2><p><b>Question</b></p><p>I
recently came across the metamath "minimize" feature. It seems to me
that one can only use it with the proof one is just working on. But
supposing one adds a new theorem and one wants to minimize the proofs
of the existing theorems, is there a way to do that in a – let's say –
batch style? <a class="local" href="frl">frl</a> 6-May-2006</p><p><b>Answer</b></p><p>There
is no built-in automatic way. The following is one way to do it with
commands typed in at the MM&gt; prompt. 'xxxx' is the new theorem name.
You can also reformat the 'show label' output with a Unix script
instead of the 'tools' commands if you prefer.</p><p>You can put parts
of this procedure in a command file for the 'submit' command, with the
restrictions: (1) you can't 'submit' from inside of a command file and
(2) the 'submit' command doesn't take arguments, so you can't send the
the theorem name to it. [Both of these features could be added in
principle, but currently they are not on my to-do list for lack of
demand.]</p><pre>  open log 1.tmp
  show label */linear
  close log
  tools
  match 1.tmp $p ''
  delete 1.tmp '' ' '
  delete 1.tmp $ ''
  add 1.tmp 'prove ' '&amp;minimize xxxx&amp;save new_proof/compressed&amp;quit'
  substitute 1.tmp '&amp;' '\n' all ''
  quit
  submit 1.tmp</pre><p>At
the end, 'show usage xxxx' will tell you which proofs were reduced.
Alternately, before 'submit 1.tmp', I like to 'open log 2.tmp'. Then I
can search the log for the word 'decreased' to see how much the proofs
were reduced. --<a class="local" href="norm">norm</a> 6-May-2006</p><h2>Nat.mm is nearly finished</h2><p>In fact I have succeeded in importing <a class="url outside" href="http://www.cs.cmu.edu/%7Efp/courses/atp/handouts/atp.pdf">Frank Pfenning's axioms</a>.
I only simplified predicate calculus ones ( I removed the substitution
operator ) but it is not an important modification, we could just have
left the existing axioms but it would have been less smooth.
Propositional calculus is imported with no modification. As you know I
didn't succeed in finding a set of axioms to implement the syntactical
definition of bondage ( I still think it should be possible however )
but the new axiom which uses the '(ph -> A. x ph )' statement works
perfectly. There is only two points that I can't understand in
Pfenning's set of axioms. 1) He gives two axioms to deal with equality
(in fact <a class="url outside" href="http://us2.metamath.org:8888/mpegif/stdpc6.html">stdpc6</a> and <a class="url outside" href="http://us2.metamath.org:8888/mpegif/stdpc7.html">stdpc7</a>)
but from these two axioms I think we can't expect to derive back set.mm
axioms about equality. Therefore I guess the only point on which set.mm
really differs from a textbook set of axioms concerns its way to manage
equality : could you tell us more about it ?</p><dl class="quote"><dt></dt><dd>I assume you have read <a class="url outside" href="http://us2.metamath.org:8888/mpegif/mmset.html#traditional">Appendix 3: Traditional Textbook Axioms of Predicate Calculus with Equality</a>.
We can add stdpc6 and stdpc7 as axioms - and it would probably be a
good idea, to help the user correlate your work with Pfenning's - but
they will not be sufficient. The problem is again (just as for variable
binding) that the traditional (Pfenning's) system involves
metamathematics about the <em>structure</em> of a formula in its concept of "free for", whereas the axioms in set.mm deal only with the <em>logical</em>
properties of a formula. So, "free for" is expressed with a logically
(but not structurally) equivalent representation involving set.mm's
proper substitution notation, as you can see in stdpc7. The advantage
of doing this is that we need no "external" metamathematics, e.g. in
the proof checker, to prove general-purpose metatheorems from the axiom
schema. This is what the "meta" in "Metamath" is all about, and it
makes Metamath's proof-checking engine very simple. The disadvantage,
though, is that the implicit metalogic in the textbook schema
effectively has to be performed explicitly in the proof itself to
arrive at logically equivalent results, and we need a bunch of
technical axioms, ax-8 through ax-16, to accomplish that. Now if you
add stdpc6 and stdpc7 to ax-8 through ax-16, it is very possible that
several of ax-8 through ax-16 will become redundant - and it would be
an interesting exercise to try to determine which ones - but most
likely not all of ax-8 through ax-16 can be eliminated.</dd><dt></dt><dd>.<dl class="quote"><dt></dt><dd>Thank
you Norm for all these answers. There are many different things and I
shall try to elucidate them during the next week. Obviously there is
still something that I don't understand about the exact nature of
metamath. I think there is something that could help me: giving a
definition of metamath logic in formal terms (very formal in the most
'metamathical' fashion in fact) ant trying to compare it with a formal
textbook definition of logic. I will try too. The things I hope to
grasp more deeper this way is</dd><dt></dt><dd>.</dd><dt></dt><dd>1) the relationship of metamath with the logical systems you describe in your paper</dd><dt></dt><dd>.<dl class="quote"><dt></dt><dd>I
wouldn't worry too much about that, since most of the paper focuses on
$d-free systems. The system S3' (set.mm's) is basically an extension
adding in the $d's to make it more flexible.</dd></dl></dd><dt></dt><dd>.</dd><dt></dt><dd>2) the absence (?) of schemas in metamath and their presence in textbook logic,</dd><dt></dt><dd>.<dl class="quote"><dt></dt><dd>Metamath's "axioms" (a misnomer) are really <em>schemas</em>
just like the schemas of textbook logic. Try to think of them in that
way. For prop. calc., these schemas are the same. The main difference
in pred. calc. is that the metalogical notions the schemas are allowed
to use are more restricted than in textbook logic - e.g. no explicit
"free variables" - and as a result we need more schemas for
completeness. But each Metamath schema can be (metalogically) derived
from textbook schemas. In fact I strongly recommend doing this as a
pencil and paper exercise - from Hilbert-style textbook schemas, not
Pfennings, to start with - so that you can see the connection. If you
get stuck, even on the first one if you don't "get" the idea, post here
and I'll help you work through it.</dd></dl></dd><dt></dt><dd>.</dd><dt></dt><dd>3) The fact that individual variables are distinct in normal logic system and the fact they are not distinct in metamath</dd><dt></dt><dd>.<dl class="quote"><dt></dt><dd>That
Metamath's variables are not distinct by default is merely a
convention. An alternate equivalent language could be devised where
instead of specifying $d's for distinct variables, we specify say $n's
for ones that don't have to be distinct.</dd><dt></dt><dd>.</dd><dt></dt><dd>My
choice of not having $d's in most of the axioms in set.mm was also
arbitrary. It was mainly done (1) to show how far we could get without
$d's, (2) to make the axioms more compact by effectively combining two
cases into one, e.g. E. x x = y and E. x x = x, and (3) because the
technical simplicity of a $d-free system, with no free/bound variable
conflicts ever to worry about, appealed to me. Perhaps this was a poor
design decision from a human point of view, I don't know. But it is
certainly possible to have a logically complete set.mm axiom system
with $d's on all set variables in all axioms. Then, if we want
metalogical completeness (optional, not a logical necessity), we'd add
one or more axioms to give us all the non-$d cases. This would a kind
of reverse of what we now do with ax-16. And, provided we choose the
metalogical completeness option, we could derive each system from the
other in Metamath itself. (If we don't choose that option, we could
still derive them from each other, but with informal metalogic outside
of Metamath.)</dd><dt></dt><dd>.</dd><dt></dt><dd>The <a class="url outside" href="http://us2.metamath.org:8888/mpegif/mmnotes.txt">mmnotes.txt</a>
entry for 11-May-06 also has a related discussion about informal
textbook notation P(x) to indicate that x is a free variable in wff P.</dd></dl></dd><dt></dt><dd>.</dd><dt></dt><dd>4) the use of metalogic in proof-checkers other than metamath. – <a class="local" href="frl">frl</a> 19-May-06</dd><dt></dt><dd>.<dl class="quote"><dt></dt><dd>Only
a few of them allow wff metavariables (Isabelle does, Otter doesn't). I
don't know how it is usually implemented, but I would guess it is
probably not a natural fit but instead an awkward add-on to the main
algorithm, perhaps simulating wff variables with temporary predicate
constants. In Otter, the issue has been avoided in one of its set
theory developments by using finitely axiomatized NBG set theory. – <a class="local" href="norm">norm</a> 20-May-2006</dd></dl></dd></dl></dd></dl><p>2) Pfenning doesn't give axioms for the 'belongs to' operator.</p><dl class="quote"><dt></dt><dd>('Belongs-to
operator' is usually called the 'membership relation'.) Actually, he
does. All of the equality axioms for the membership relation follow
immediately from the equivalent of stdpc7 in Pfenning's system. To see
this, write down one of the membership equality axioms, and look very
carefully at the definition of "free for", keeping in mind that the
membership relation is a binary predicate symbol. But to derive them
from set.mm's version of stdpc7, you almost certainly need some of the
technical axioms ax-8 through ax-16, but offhand I don't know which
ones.</dd></dl><p>In set.mm you say that it is possible to give a
definition of equality using the 'belongs to' operator. But is it
possible to omit the 'belongs to' axioms and to define it using
equality ? --<a class="local" href="frl">frl</a></p><dl class="quote"><dt></dt><dd>No, it is not possible to define the membership relation in terms of equality. --<a class="local" href="norm">norm</a> 18-May-2006</dd></dl><p>P.S.: In the htmldef statements you use a _wff.gif image that I can't find in symbol.zip. Where is it ?</p><dl class="quote"><dt></dt><dd>symbols.zip has only standard black symbols. Colored and custom symbols such as <a class="url outside" href="http://us2.metamath.org:8888/mpegif/_wff.gif">_wff.gif</a> are found in the mpegif directory, and their names almost always start with an underscore ('_'). --<a class="local" href="norm">norm</a> 15-May-2006<dl class="quote"><dt></dt><dd>Ah thank you.</dd></dl></dd></dl><h2>Metamath.pdf Spec Change Proposal</h2><pre>    Problem:  a Metamath formula's exact meaning is presently
    ========  unresolved without access to the variable
              hypotheses in scope of the statement where
              the formula occurs.</pre><pre>              To determine the Type Code of a variable
              in a formula, which is a necessary
              determination for parsing, it is necessary
              to perform a lookup to find the associated
              Variable Hypothesis, which then provides
              the associated Type Code.(1)</pre><pre>    Solution: This extra lookup step could easily be
    ========= avoided by programs operating on Metamath
              databases if:</pre><pre>              a) Internally formulas can be defined to
                 consist of Constants and Variable
                 Hypothesis Labels (instead of Constants
                 and Variables);</pre><pre>              and</pre><pre>              b) The Metamath.pdf specification is changed
                 to outlaw a Variable Hypothesis label that
                 is the same as any Constant in the Metamath
                 database.</pre><pre>             Because a Metamath Variable Hypothesis can refer
             to no more than one Variable, regardless of
             whether the Variable Hypothesis is Active or
             Inactive, even if subsequently re-activated,
             a Variable Hypothesis label is unambiguous about
             which Variable is being referred to. The converse
             is not true, as in theory a single Variable can
             be referred to by multiple Variable Hypotheses
             of different Types.</pre><pre>    Impact:  The likely impact on existing Metamath files
    =======  is negligible. It is highly unlikely that anyone
             has created a Metamath file containing a Variable
             Hypothesis label that is the same as a Constant.</pre><pre>             A trivial change to existing programs that validate
             Metamath files will be needed, if full conformity
             to the Metamath.pdf specification is to be maintained.</pre><pre>             Retrofitting existing code such as mmj2 to
             immediately take advantage of this specification
             change is unlikely. Rather, this change is
             proposed for the benefit of future programs.</pre><pre>             Note also that there are workarounds in the event
             that this proposal is not accepted as given. For
             example, a formula could be stored as an array
             of object references without regard for labels
             and symbols. This proposal is intended merely
             as a grammatical restriction to provide
             flexibility for future uses of Metamath.</pre><pre>    Footnotes:
    ==========
    1) During initial file load mmj2 takes advantage of
    the specification requirement that each variable
    reference must refer to a variable declaration that is
    Active at the point of reference, and that, at that
    point in the input Metamath file/database there must
    be an Active Variable Hypothesis pointing to the
    variable -- inside the mmj.lang.var.java object a
    reference to the currently activeVarHyp is stored.
    However, post-initial load the activeVarHyp reference
    is useless.</pre><p>--<a class="local" href="ocat">ocat</a> 21-May-2006</p><p>What
is it with you computer science people? First, Raph had to do a
workaround because there used to be a theorem whose label was "1o"
(since changed) and a constant named "1o", and now this…</p><p>Maybe
it's just me, but there seems to be such a fundamental difference
between labels and math symbol tokens, down to the syntax of what
characters are legal, that I would think that it would be natural for
any parser to put them into completely different arrays from the start,
as metamath.exe does. Then, for efficiency, all internal mentions of
labels and math tokens would be numerical pointers to positions in
those arrays, and the corresponding strings never used again (except
for display purposes). But I'm no computer scientist. :)</p><p>What if
I just make the following universal prohibition at the end of 4.1.1, to
make everyone happy: "No label token may match any math symbol
token.\footnote{This restriction did not exist in earlier versions of
this specification. While not theoretically necessary, it is imposed to
make it easier to write certain parsers.}"</p><p>Discuss. --<a class="local" href="norm">norm</a> 21-May-2006</p><p>Norm,
I agree with your proposal. It amounts to specifying that Metamath
Constant, Variable and Statement Labels share a single namespace.</p><p>Another
rationale for the specification change is supporting the eventual
converion of .mm files to relational databases – every "math object"
will have a unique name. And, as mentioned, a formula can be stored
unambiguously as follows:</p><pre>    |- ( wph -&gt; wps )</pre><p>Just as a side note, the idea of storing set.mm in a fully normalized relational database is not as crazy as it may sound.</p><p>The
.mm format in plain text files requires continual re-validation to make
sure that everything remains consistent. That means re-verifying all
proofs and re-parsing formulas pretty much all the time just to be
safe. But with a database safeguarding the data, with the help of
custom code on top of the RDBMS, that can be avoided. And the full set
of modern tools for querying and manipulating databases becomes
immediately available. One immediate need would be to extract into .mm
format for the benefit of the old "legacy" programs.</p><p>It is also
likely that availability of Metamath "databases" in real databases will
stimulate the imagination of new users, who may be motivated to provide
new activities involving Metamath systems.</p><p>One reason, perhaps,
why Metamath has not become wildly popular on a global basis -- aside
from the steep learning curve -- is that the paucity of activities
(i.e. "fun stuff"). After all, what can one <b>do</b> with Metamath
now except to a) prove theorems and b) read the Proof Explorer website?
The spirit-crushing magnitude of set.mm alone may deter all but the
most courageous mountain climbers…</p><p>So, we need to manufacture new
fun activities for students and adventurers, and at the same time,
somehow discover what reasons do the world's mathelogicians have for
not using Metamath. (Metamath also contains many universes for computer
scientists/programmers to explore.)</p><p>--<a class="local" href="ocat">ocat</a> 23-May-2006</p><p>I
will wait a month and then, based on the discussion here (and the mood
I'm in that day :), decide whether to go ahead with the shared
namespace spec change. I probably will go ahead with it.</p><p>--<a class="local" href="norm">norm</a> 24-May-2006</p><p>I
think it makes sense to do, and may be very useful in certain
programming contexts, though not all. In the long run it will be
helpful for the VR input method handler to know that each "gesture" is
unique :)</p><p>If I were to implement the change in mmj2 I would most
likely combine the Symbol table and Statement table to avoid double
lookups at file-load time.</p><p>Perhaps users not monitoring the Asteroid should be given notice at the Notes section of metamath.org.</p><p>--<a class="local" href="ocat">ocat</a> 26-May-2006</p><p><b>Final disposition</b></p><p><a class="local" href="ocat">ocat</a>'s namespace proposal has been officially approved, and the on-line <a class="url outside" href="http://us2.metamath.org:8888/index.html#book">''Metamath'' book</a>
has been updated (see footnote 6 on p. 94 of the Metamath book). People
with a printed copy of the book should add the sentence referenced by
that footnote. (A product recall is not planned at this time.) Version
0.07.17 of the <a class="url outside" href="http://us2.metamath.org:8888/index.html#mmprog">Metamath program</a> enforces the new namespace requirement.</p><p>This
is the second spec change since 1997. The other change was a few years
ago, requiring all labels to be unique instead of allowing $e / $f
labels to be reused, again to simplify writing parsers.</p><p>--<a class="local" href="norm">norm</a> 30-Jun-2006</p><h2>Conundrum of Extensionality</h2><p>Hi Norm,</p><p>There are a couple of things I am just not "getting", and they're tangled in a Gordian Knot.</p><p>The
context is that I am looking at free vs. bound vs. $d statements and at
notation differences between Metamath and "standard" set theory
notations.</p><p>Regarding <b>bound</b> variables, I understand that ' A. ' (for all) and ' E. ' (there exists) bind variables so that they do not occur <b>free</b> in formulas. ' A. x -. x e. x ' binds ' x ' – ' x ' does not occur <b>free</b> in ' -. x e. x '.</p><p>I
interpret that as meaning that the name of a bound variable 'x' does
not matter, another variable name could be used and the meaning of the
formula would stay the same. Just as in programming, 'i' could just as
well be 'j' in</p><pre>    ' for (int i = 1, i &lt; n, i++) { blah } '.</pre><p>If 'n' were changed then the meaning would change because 'n' occurs <b>free</b> in the programming statement. But the <b>name</b>, 'i', is irrelevant in that context.</p><p>However, in the following C expression, both 'i' and 'j' are <b>bound</b>; their specific names are irrelevant but their names must be different and must be different than the names of other, <b>free</b> variables referenced in the body of the statment.</p><pre>    ' for (int i = 0, j = 1; i &lt; n; i++, j++) { blah } '.</pre><p>Now, in the Axiom of Extensionality, the English definition is "Two sets are equal if they have exactly the same elements.".</p><p>In standard set theory notation, the axiom is written:</p><pre>    A. A A. B ( A. x ( x e. A &lt;-&gt; x e. B ) -&gt; A = B )</pre><p>In that formula, 'A', 'B' and 'x' are <b>bound</b> – there are no free variables, and no mention anywhere of further restrictions (such as 'for all x free in y' or suchlike.)</p><p>The Metamath formula of the Axiom of Extensionality is:</p><pre>    $d x y z w v u t $.
    ax-ext $a |- ( A. z ( z e. x &lt;-&gt; z e. y ) -&gt; x = y ) $.</pre><p>So now I have three conundrums:</p><p>1) Why isn't the Metamath formula written:</p><pre>    $d x y z w v u t $.
    ax-ext $a |- A. x A. y ( A. z ( z e. x &lt;-&gt; z e. y ) -&gt; x = y ) $.</pre><p>I assumed that the prefix 'A. x A. y' are not necessary because of the $d specifications… but if so, then…</p><dl class="quote"><dt></dt><dd>No,
it doesn't have to do with the $d's. The versions with and without the
prefix 'A. x A. y' are equivalent: ax-gen can add it to set.mm's
version, and ax-4/ax-mp can remove it. I chose to remove it to shorten
the axiom. Some texts add it obtain wffs with no free variables (called
"statements"), either as a matter of style or to be compatible with
predicate calculus axiomatizations that do not allow free variables.</dd></dl><p>2) Why can't it just be written as:</p><pre>    $d x y z w v u t $.
    ax-ext $a |- ( ( z e. x &lt;-&gt; z e. y ) -&gt; x = y ) $.</pre><dl class="quote"><dt></dt><dd>Because
this isn't true for all x,y,z. E.g. let x = 0, y = {0}, z = {{0}}. Then
the antecedent is 'false &lt;-> false' i.e. true, but the consequent is
false.</dd></dl><p>ALSO…</p><p>3) Why are there so many $d restrictions on ax-ext? Wouldn't it be sufficient to write:</p><pre>    $d x z $.
    $d y z $.
    ax-ext $a |- ( A. z ( z e. x &lt;-&gt; z e. y ) -&gt; x = y ) $.</pre><dl class="quote"><dt></dt><dd>Yes, it would. In fact this "stronger" version is proved as <a class="url outside" href="http://us2.metamath.org:8888/mpegif/zfext2.html">zfext2</a>
(whose proof I think you will find instructive). I am simply following
the usual textbook convention of stating the starting axioms with all
variables distinct. There is also the opposite extreme of stating them
with <a class="url outside" href="http://us2.metamath.org:8888/mpegif/mmzfcnd.html">no $d's</a>. :) – <a class="local" href="norm">norm</a> 8-Jun-2006</dd></dl><p>--<a class="local" href="ocat">ocat</a> 8-Jun-2006</p><p>I'm having a hard time translating ax-ac into a concise yet faithful English sentence:</p><pre>    ax-ac $a  |- E. y
                    A. z
                       A. w
                          ( ( z e. w /\ w e. x ) -&gt;
                            E. v
                               A. u
                                  ( E. t
                                       ( ( u e. w /\ w e. t ) /\
                                         ( u e. t /\ t e. y ) )
                                    &lt;-&gt; u = v
                                  )
                          ) $.</pre><p>--<a class="local" href="ocat">ocat</a> 19-Jun-2006</p><p><b>Answer</b></p><p>"Given
any set x, there exists a y that is a collection of unordered pairs,
one pair for each non-empty member of x. One entry in the pair is the
member of x, and the other entry is some arbitrary member of that
member of x."</p><p>Actually, I don't think it's worth trying to
understand its raw axiomatic form. It is simply the shortest equivalent
to the longer standard axiom that I was able to come up with by
applying logical transformations without regard to meaning. Perhaps an
analogy is <a class="url outside" href="http://us2.metamath.org:8888/mpegif/meredith.html">Meredith's axiom</a>,
whose purpose is just to show a shortest possible single axiom for
propositional calculus: you "understand" it by showing its equivalence
to axioms that make sense to humans.</p><p>You may want to look at <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ac3.html">ac3</a>, which is ax-ac restated with abbreviations and is accompanied by a detailed explanation as well as a concrete example.</p><p>The standard textbook version of AC is derived as <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ac8.html">ac8</a>.</p><p>--<a class="local" href="norm">norm</a> 19-Jun-2006</p><h2>Substitutability WRT Named Typed Constants and $d</h2><p><b>Question</b></p><p>A
rarely considered fact is that $d restrictions only apply to variables
substituted into variables; any constants inside the expressions
substituted in are ignored, even if these are Named Typed Constants
representing actual classes.</p><p>I have not yet satisfied myself
regarding the rationale behind this. Obviously, it makes sense to
ignore punctuation/delimiter constants such as "(", when verifying
distinct variable restrictions, but the reasoning is not so clear when
constants naming actual class objects are considered. In effect, a
class object could be substituted into two variables subject to
distinct variable restrictions, and no error message would result. --<a class="local" href="ocat">ocat</a></p><p><b>Answer</b></p><p>Here is one way to think about it. Whenever you have a theorem with $d x A, it is <em>logically</em>
correct to substitute for A any class expression in which all variables
are bound (although Metamath won't let you do this directly when the
expression contains x, even if x is bound.)</p><p>All set variables in
the definition of a Named Typed Constant must be bound. For example,
"(/)" is defined as "{ x | x =/= x }", where x is bound. Indeed, this
is a requirement for the definition to be sound.</p><p>Now, consider
the theorem "E. x x = { A }" (the singleton of any class exists). We
need $d x A for this to hold - otherwise, we could conclude E. x x = {
x }, which is false in ZF set theory. Now, replace A with (/), to
obtain E. x x = { (/) } with no $d. Then, replace (/) with { x | x =/=
x }, to obtain E. x x = { { x | x =/= x } }, again with no $d. But no
$d is necessary, since x is bound on the rhs of the =. In fact, this
can be a useful trick to get rid of unnecessary $d's: in the original
theorem, we are not allowed to substitute { x | x =/= x } for A at all,
and we are not allowed to substitute { y | y =/= y } for A unless we
specify $d x y. --<a class="local" href="norm">norm</a> 27-Jun-2006</p><p>OK, that makes perfect sense now. A Named Type Constant or its underlying definition can <b>logically</b>
be properly substituted for a class variable because there are no side
effects with variables in the enclosing scope (of quantification.) Done
deal as far as I am concerned. And the definition for a Named Typed
Constant must not contain any free variables or else it will not be a
sound definition.</p><p>I will need to work more on substitutability, squaring this with this language: <a class="url outside" href="http://en.wikipedia.org/wiki/Mathematical_logic#Substitutability">Substitutability</a> (a bit more restrictive – "y does not occur in t and t is substitutable for x in a").</p><p>Getting closer. I'm dangerous now :) --<a class="local" href="ocat">ocat</a> 27-Jun-2006</p><p>By the way, an equivalent way to say "t is substitutable for x in a" used by many books is "x is free for t in a".</p><p>That
Wikipedia page illustrates the complexity of the metalogic of the
traditional approach to predicate calculus, when it is fully
formalized. The difficulty of verifying proofs using that metalogic was
one of the motivations for the different, but logically equivalent,
approach that Metamath uses.</p><p>The set.mm axioms are similar in many respects to the ones in Tarski's <a class="url outside" href="http://us2.metamath.org:8888/mpegif/mmset.html#Tarski">simplified formalization of predicate logic with identity</a>, and were in fact greatly influenced by them. Tarksi writes in that paper:</p><dl class="quote"><dt></dt><dd>"Two
of the notions commonly used in describing the formalism of
(first-order) predicate logic exhibit less simple intuitive content and
require definitions more careful and involved than the remaining ones.
These are the notions of a variable occurring free at a given place in
a formula and the related notion of the proper substitution (or
replacement) of one variable for another in a given formula. The
relatively complicated character of these two notions is a source of
certain inconveniences of both practical and theoretical nature; this
is clearly experienced both in teaching an elementary course of
mathematical logic and in formalizing the syntax of predicate logic for
some theoretical purposes."</dd></dl><p>Perhaps it would also be helpful to revisit the discussion of Metamath vs. the <a class="url outside" href="http://us2.metamath.org:8888/mpegif/mmset.html#traditional">Traditional Textbook Axioms of Predicate Calculus with Equality</a>.
Most of the discussion involving free variables in a wff phi also
applies to free variables in a class variable A. (In set theory, wff
(meta)variables and class (meta)variables can be viewed as two
different ways of representing the same thing, and one can be
transformed to the other. An analogy might be the time domain and
frequency domain in the theory of electrical signals.)</p><p>I should also mention that there are two <em>logically</em> equivalent (although not <em>structurally</em>
identical) ways to express the concept of "x is not free in A", and
they can be converted to each other. The first way is just $d x A, such
as in theorem <a class="url outside" href="http://us2.metamath.org:8888/mpegif/n0.html">n0</a>.
The second way is the hypothesis $e |- ( y e. A -> A. x y e. A ), where
y is a new variable with $d y x and $d y A, such as in theorem <a class="url outside" href="http://us2.metamath.org:8888/mpegif/n0f.html">n0f</a>. I think of these mentally as "weak" and "strong" ways, since it is easy to go from "strong" to "weak" (just use <a class="url outside" href="http://us2.metamath.org:8888/mpegif/ax-17.html">ax-17</a>) but hard the other way (see, for example, how <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dfss2.html">dfss2</a> is converted to <a class="url outside" href="http://us2.metamath.org:8888/mpegif/dfss2f.html">dfss2f</a>).
There is no exact match between either of these to the traditional "not
free in" notion, but we have: "strong" =&gt; traditional not-free-in
=&gt; "weak", where "=&gt;" means superset of the wffs for which the
condition holds. Examples: x isn't not-free in x = y in all three
senses; x is not-free in x = x in the "strong" sense but isn't not-free
in the traditional and "weak" senses; x is not-free in A. x x = x in
the "strong" and traditional senses but not in the "weak" sense; if $d
x y, then x is not-free in y = y in all three senses.</p><p>Since
neither of the set.mm "not free in" concepts correspond exactly to the
traditional one, it is perhaps confusing to call either of them by that
name, although both are logically equivalent to it. Sometimes I call
the "strong" one above "effectively not free in". Their advantage over
the traditional one is that they don't need a (somewhat complex)
recursive definition. But neither of them apparently appear in the
literature, so there seems to be no standard terminology for them.</p><p>--<a class="local" href="norm">norm</a> 28-Jun-2006, 30-Jun-2006</p><p>A few points:</p><p>1. <a class="url outside" href="http://www.mathsci.appstate.edu/%7Ejlh/primer/hirst.pdf">Hirst and Hirst's A Primer for Logic and Proof</a>
– Excellent. A breath of fresh air after struggling with Enderton and
friends :) I like the abundance of in-line examples for every key
concept. And the straight talk down the main path, which can be
complicated later by those with an interest in doing so :)</p><p>2. The
Metamath .mm language is conceptually more fundamental (lower) than
predicate calculus and can be used for other theories of reasoning,
which may yet be invented. In predicate calculus variable binding is
tied to the quantification operators, of which Metamath has no built-in
knowledge or logic. As an experiment it would be interesting to define
a superset of the .mm language which would include as an extra an
optional "bound ___" stipulation on syntax builder axioms (such as
"wex", say.) The proof verifier would be responsible for checking for
substitutability per the traditional notions of such <b>and</b>
verifying the $d restrictions (toggle on/off via runtime parameter.) We
would then reprocess all 10,000 Metamath theorem proofs and ponder the
results.</p><p>3. Unrelated: I googled free/bound and hit ACL2's use of the terms in its unification process. I noted with <b>extreme</b>
interest that they recommend putting the "weightiest" hypotheses first
to expedite unification – or else the combinatorial explosion problem
might prevent finding the match. In mmj2's ProofUnifier I foresaw
exactly that same problem thanks to a scan of set.mm which revealed a
theorem with 19 hypotheses. I try to avoid the problem by presorting
the hypotheses of the step to be unified, putting the longest formulas
first and if formulas have equal length, the formula that has at least
one variable in common with the unified step's formula gets precedence.
Where I think I can optimize my code is that, unless memory fails, I
should cache the result of the sort operation (which is only invoked if
needed based on local conditions). It is interesting how these sorts of
systems tend to need to solve the same problems, though sometimes in
different form. In the end we just want to get the job done in the
fastest, cleanest way possible. It may be that $d isn't all that much
simpler than the "free/bound" concept of substitutability but if it
works better then we need to respect that and pay attention to what
that means.</p><p>--<a class="local" href="ocat">ocat</a> 30-Jun-2006</p><h2>I really dislike the existential quantifier</h2><p><b>Question</b></p><p>How can I substitute x by another variable in an antecedent .</p><p>For instance I have |- ( ph -> ( E. x ps -> ch ) )</p><p>and I want |- ( ph -> ( th -> ch ) )</p><p>with somewhere |- ( x = y -> ( ps &lt;-> th ) )</p><p>Is it possible to do that ? – <a class="local" href="frl">frl</a> 27-Sep-2006</p><p><b>Answer</b></p><p>I believe you want to apply <a class="url outside" href="http://us.metamath.org/mpegif/cla4ev.html">cla4ev</a> with <a class="url outside" href="http://us.metamath.org/mpegif/syl5.html">syl5</a>. To go the other way, use <a class="url outside" href="http://us.metamath.org/mpegif/19.23adv.html">19.23adv</a>,
provided x doesn't occur in ph and ch. This, by the way, is one of the
ways to emulate the "C rule" trick of traditional logic, where you get
rid of an existential quantifier by introducing an imaginary constant
to replace the quantified variable.</p><p>The following situation is
very common: we want to prove ( ph -> ( E. x ps -> E. y ch ) ). We are
able to prove ( ph -> ( ps -> th ) ), where th has x in it (basically th
is a special case of ch that we are able to find assuming ps). First,
we change this to ( ph -> ( ps -> E. y ch ) ) using cla4ev. Then we
change it to ( ph -> ( E. x ps -> E. y ch ) ) using 19.23adv. An example
of this is shown in step 21 of <a class="local" href="raph">raph</a>'s <a class="url outside" href="http://us.metamath.org/mpegif/r1pwcl.html">r1pwcl</a>.
I personally feel that this technique is easier to understand than the
"C rule," but perhaps it is just because I am used to it.</p><p>The restricted quantifier versions of cla4ev and 19.23adv are <a class="url outside" href="http://us.metamath.org/mpegif/rcla4ev.html">rcla4ev</a> and <a class="url outside" href="http://us.metamath.org/mpegif/r19.23adv.html">r19.23adv</a>.</p><p>--<a class="local" href="norm">norm</a> 27 Sep 2006</p><hr><h2>Equivalence of set.mm's CC and "the" complex numbers</h2><p><em>Preface:
I'm sure this is a naive comment. I am not a mathematician. I'm
probably not even using the word "model" properly. Oh well. --jorend</em></p><p>I have heard that someone is claiming that <a class="url outside" href="http://us.metamath.org/mpegif/2p2e4.html">2 + 2 = 4</a>. Not wanting to seem gullible, I have looked into it, and I have some questions.</p><p>set.mm contains a set-theoretic construction of (you could say: a <em>model</em> of) the complex numbers. It proves the <a class="url outside" href="http://us.metamath.org/mpegif/mmcomplex.html#axioms">axioms for complex numbers</a>, which I will call "the postulates" here, from set theory and <a class="url outside" href="http://us.metamath.org/mpegif/df-c.html">df-c</a>. It seems to me this proves only that Metamath's set-theoretic construction of the reals is <em>at least</em>
as strong as the postulates. How do I know it isn't stronger? In other
words, you might prove something about Metamath's complex numbers that
follows from set theory and the particular definition involved, but
which doesn't necessarily follow directly from logic and the
postulates. In particular, it seems possible that set.mm's theorem "2 +
2 = 4" does not follow from the postulates.</p><p>You could avoid this danger by using set theory <em>only</em>
to prove the postulates, and then using only the postulates (and not
any theorems involving the underlying set-nature of the model) from
that point forward. But I don't think this is the case for the proof of
2p2e4. I clicked on opreq2i and immediately got into trouble.</p><p>I
imagine there's a theorem of model theory that all models of the
complex numbers are isomorphic in some relevant sense (and the same for
functions, ordered pairs, etc.); and some sort of metatheorem that says
that two isomorphic systems have all the same theorems and proofs. But
I don't see anything along these lines in set.mm… am I wrong?</p><p>I'll withhold judgement on 2p2e4 until these questions are settled. --<a class="local" href="jorend">jorend</a> 12 Oct 2006</p><p><b>Answer</b></p><p>The
construction of complex numbers in set.mm is done in a stand-alone
section that leads to the "postulates." The postulates themselves are
independent of the construction in the sense that we could remove the
Dedekind-cut construction that's there now and replace it with a
Cauchy-sequence construction or any other.</p><p>When set.mm was first
being built, the postulates were in fact true axioms, and a good
percentage of the theorems there now about complex numbers were proved
using them, as a theory extending ZF set theory. The object CC was
primitive, with the only assumptions made about it being those stated
in these axioms. Later, when the Dedekind-cut construction was
completed, CC was changed from a primitive to a defined object, and the
axioms were replaced by theorems (now called "postulates"), with no
change to any of the complex number proofs that used them.</p><p>The
real number developments in textbooks use "axioms" that presuppose and
go on top of set theory. In elementary books, this fact may be unstated
and informal, although without some set theory you can't get too far
with the completeness axiom.</p><p>There may be real number systems
that go directly on top of logic (not first-order, but second or higher
order) and presuppose no set theory, but I am not familiar with them,
though, and someone else might have something to say about it. I don't
know how such a system would state or work with the completeness axiom.</p><p>The
theorem opreq2i you mention is a theorem of set theory, but it is not
part of the complex number construction, and a formalization that adds
complex numbers as axioms would still use it. It was used in the
original set.mm when the complex number postulates were still axioms.</p><p>An
approach that would be "purer" in some sense would be to define the
class of all complex number systems, just as set.mm now defines the
class of all topologies or the class of all metric spaces. At a future
date perhaps such a class might be defined for some purpose (studying
sets isomorphic to complex numbers, etc.), and the present construction
would provide a proof that the class of all complex number systems has
at least one member. However, it is much more efficient to prove
theorems about a fixed object CC, rather than have every theorem begin
with the hypothesis "Let (CC,…) be a complex number system…"</p><p>How
do we know that the theorems about complex numbers aren't using some
construction-dependent property, that wouldn't hold for some other
construction? Metamath does not ensure that directly, but to verify it
we would remove the construction and replace the postulates with true
axioms. All proofs should still be correct.</p><p>I went to some
trouble to avoid referencing the construction even when it was tempting
to do so. For example, when defining +oo (plus infinity), we need a
"new" object. To make this construction-independent, I defined it in
terms of CC itself, leading to the requirement that we use the Axiom of
Regularity, not otherwise used in the construction of reals, to prove
that the object is "new". I saw no way to avoid that axiom and also
have a construction-independent +oo.</p><p>--<a class="local" href="norm">norm</a> 13 Oct 2006</p><p>Thank you, Norm. What a kind and thought-provoking answer. I meant to put a smiley ;) at the end of the question, by the way.</p><p><em>"real number systems … directly on top of logic"</em>: Oh, yes, I completely agree. No, I didn't mean to point in that direction.</p><p><em>"the class of all complex number systems"</em>: Interesting! I hope to follow up on this later.</p><p><em>opreq2i</em>: Yes, I see now. Oops.</p><p>--<a class="local" href="jorend">jorend</a> 13 Oct 2006</p><p>There
is at least one other way to look at this question: a discipline of
splitting a set.mm-like library into modules, and being careful about
which modules import what. In particular, you'd have an interface
containing "the axioms of complex numbers," and your set-theoretic
construction of these numbers (i.e. df-c and the cloud of theorems
supporting it) would export this interface. Then, other theorems like
2p2e4 would import the abstract complex number interface and not be
able to reference theorems specific to the construction.</p><p>One
excellent way to keep the designer of such interfaces honest is to
provide at least two different constructions. For example, importing
HOL should give access to Harrison's construction of the reals, which
is based on entirely different principles than the usual set-theoretic
Dedekind cut construction, but should nonetheless be able to export the
same abstract complex number interface.</p><p>I'm sure there's a fairly
deep metatheory around proof modules with interfaces, but frankly it's
not of burning interest to me right now. I'm sure you could do a
mechanical translation where all of the constants imported become
variables instead, and the set of axioms imported is replaced by a big
giant antecedent of the form "for all instances of the variables
satisfying axiom 1, axiom 2, …, axiom n", and the result would be
fairly similar to the "class of all complex number systems" proposed by
Norm above. But I'm not really sure how much you gain from this
exercise. It may be possible to derive essentially the same results
with less pain by using a generic theory of isomorphism, so then you
just say "for all systems isomorphic to the one construced in df-c".</p><p>What <em style="text-decoration: underline; font-style: normal;">is</em>
interesting to me right now is developing a port of set.mm with the
kind of module structure I outlined above. Ghilbert Pax is the
beginning of such an effort, but it's not quite ready as the basis to
do "real work" yet. I'm getting a bit sidetracked on the development of
Peano arithmetic, which is of course powerful in its own right because
it includes all recursive functions. Even so, it should be possible to
jump ahead and develop an interface for complex numbers, to try out the
ideas sketched above.</p><p>--<a class="local" href="raph">raph</a> 13 Oct 2006</p><p>I think interfaces provide exactly the sort of abstraction I'm looking for.</p><p>…which
is troubling in a way, because it means interfaces are far more
powerful than I had thought. They're not just a harmless artifact of
the programming side of the house. They have semantic consequences.</p><p>Still, I think I like them.</p><p>--<a class="local" href="jorend">jorend</a> 18 Oct 2006</p><h2>"Peano's arithmetic is inconsistent"</h2><p>I
have just been reading this sentence on a newsgroup. I'm not a
specialist but it sounds so strange. So is Peano's arithmetic
consistent? (in that case very bravely I will correct the mistake of my
French friend :-) ). – <a class="local" href="fl">fl</a></p><p>It
is safe to say that Peano's arithmetic has not been proven
inconsistent, as such a proof would revolutionize mathematics and make
major headlines in the news. On the other hand, any theory that is able
to express elementary arithmetic cannot be both consistent and
complete, so we can't say that Peano's arithmetic is consistent,
either, although it seems to be a pretty safe assumption from a
practical point of view. See <a class="url outside" href="http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorem">Gödel's incompleteness theorem</a>. – <a class="local" href="norm">norm</a> 31 Oct 2006</p><p>Thanks
for the reference. By the way I've found the original article by Peano
in a book. Very impressive (the birth act (???) of logic in some way).
It's in latin. There are about 90 propositional axioms (certainly the
largest propositional axiomatic in the world :-) ) and there are axioms
that look like '(a e. N -> a = a )'. – <a class="local" href="fl">fl</a> 1-Nov-2006</p><p>Also of interest is <a class="url outside" href="http://en.wikipedia.org/wiki/Self-verifying_theories">self-verifying
theories</a>
that (necessarily) cannot contain Peano arithmetic due to Gödel's
incompleteness theorem, but are sufficiently strong that they can prove
their own consistency. These have been studied by <a class="url outside" href="http://www.cs.albany.edu/FacultyStaff/profiles/willard.htm">Dan
Willard</a>,
and much of the work seems relatively recent (after the year 2000).
According to the Wikipedia article, "there are self-verifying systems
capable of proving the consistency of Peano arithmetic." I know very
little about this field, so I can't say much more, but it sounds quite
interesting. – <a class="local" href="norm">norm</a> 1 Nov 2006</p><p>There was a message about self-verifying theories on FOM list: <a class="url" href="http://www.cs.nyu.edu/pipermail/fom/2006-November/011095.html">http://www.cs.nyu.edu/pipermail/fom/2006-November/011095.html</a>. <a class="local" href="michael">michael</a> 12 Nov 2006</p><p>One
thing I don't understand is that if a self-verifying theory can also
prove the consistency of Peano arithmetic, doesn't that make Gödel's
second incompleteness theorem irrelevant? I must be missing something
fundamental in the big picture, because this would seem to be
astonishing news on par with the second theorem itself, but I had never
heard of it until I stumbled across mention of "self-verifying
theories" quite by accident. Perhaps someone more knowledgeable can
answer. – <a class="local" href="norm">norm</a> 13 Nov 2006</p><h2>Question about distinct variable restrictions</h2><p><b>Question</b></p><p>I'm
wondering about the necessity for the disjoint variables statements of
a $p assertion to contain $d pairs that involve variables which are not
mandatory for the assertion.</p><p>If the paragraph on "Verifying
Disjoint Variable Restrictions" in section 4.1.4 of the Metamath
document were changed to the following:</p><pre>   If two variables replaced by a substitution exist in a mandatory $d statement
   of the assertion referenced, the two expressions resulting from the
   substitution must meet satisfy the following conditions. First, the two expressions
   must have no variables in common. Second, each possible pair of
   variables, one from each expression, in which both variables are mandatory for the
   $p statement containing the proof, must exist in an active $d statement of
   the $p statement containing the proof.</pre><p>What
would break? The non-mandatory disjoint variable restrictions of an
assertion play no role when the assertion is used in subsequent proofs…
– <a class="local" href="dank">dank</a> 7 Jan 2007</p><hr><p><b>Answer</b></p><p>Hi Dan,</p><p>This
is an interesting proposal. As you know, the Metamath spec currently
requires that $d's for dummy variables be mentioned explicitly.</p><p>Both <a class="local" href="Ghilbert">Ghilbert</a> and Marnix's <a class="local" href="Hmm">Hmm</a>
verifier for Metamath make this requirement optional, but I think there
is a subtle difference in what you are proposing. I believe those
programs make the <i>implicit</i> assumption that dummies are distinct
from all other variables and still perform the checking implied by the
spec. The main intent of their approach is to avoid cluttering the
database with those $d's (or Ghilbert's equivalent) that can be
inferred from context.</p><p>What you are suggesting is that we can
skip the distinct variable checking of dummy variables completely,
since the necessary requirement will be automatically enforced by the
requirement "First, the two expressions must have no variables in
common."</p><p>As for the spec itself, the current version (for reference) is:</p><dl class="quote"><dt></dt><dd><em>Each
substitution made in a proof must be checked to verify that any
disjoint variable restrictions are satisfied, as follows. If two
variables replaced by a substitution exist in a mandatory $d statement
of the assertion referenced, the two expressions resulting from the
substitution must meet satisfy the following conditions. First, the two
expressions must have no variables in common. Second, each possible
pair of variables, one from each expression, must exist in an active $d
statement of the $p statement containing the proof.</em></dd></dl><p><a class="local" href="marnix">marnix</a>
has pointed out that clause "First, the two expressions must have no
variables in common" is automatically satisfied by "Second,…" since a
$d of the form "$d x x" (the same variable for both arguments) is
disallowed elsewhere in the spec, thus automatically satisfying the
"First,…" clause. Thus, the spec could be simplified to</p><dl class="quote"><dt></dt><dd><em>Each
substitution made in a proof must be checked to verify that any
disjoint variable restrictions are satisfied, as follows. If two
variables replaced by a substitution exist in a mandatory $d statement
of the assertion referenced, the two expressions resulting from the
substitution must meet satisfy the following condition. Each possible
pair of variables, one from each expression, must exist in an active $d
statement of the $p statement containing the proof.</em></dd></dl><p>While
I have been considering a footnote to point this out - it may speed up
some verifiers - my idea was to leave the "First,…" clause in for
clarity, even though it is theoretically redundant.</p><p>What you are proposing is</p><dl class="quote"><dt></dt><dd><em>Each
substitution made in a proof must be checked to verify that any
disjoint variable restrictions are satisfied, as follows. If two
variables replaced by a substitution exist in a mandatory $d statement
of the assertion referenced, the two expressions resulting from the
substitution must meet satisfy the following conditions. First, the two
expressions must have no variables in common. Second, each possible
pair of variables, one from each expression, in which both variables
are mandatory for the $p statement containing the proof, must exist in
an active $d statement of the $p statement containing the proof.</em></dd></dl><p>Basically, the distinct variable requirements for <i>dummy</i>
variables will automatically be satified by the "First,…" clause,
making it unnecessary to do the checking for them required by the
original spec's "Second,…" clause. So in this case, unlike in Marnix's
proposal, the "First,…" clause becomes essential.</p><p>The reason your
proposal works is that we aren't allowed to make substitutions into
proof steps to derive new proof steps from them; instead, each earlier
proof step must be used as-is towards the derivation of later proof
steps. So, the $d's on dummies are implicitly satisfied as soon as the
substitution into an external theorem is done (with just the checking
for no variables in common), and we never have to check them explicitly
since they eventually get dropped.</p><p>This is clever, and I hadn't thought of it before. Have you looked at what speedup might result from not checking dummy $d's?</p><hr><p>As
for changing the spec, I don't like to do that without a lot of thought
and other input (I think there has been only one important change since
1997). Since set.mm would be compatible with your change - the $d's on
dummies would just become redundant - there is time to think things
over, even if you write a verifier using that approach. Or, for
Ghilbert, it seems you are home free with its spec as-is - your
approach would just be a speedup.</p><p>As a spec change, your proposal
makes the spec a little less direct, since we need to have a
theoretical justification that no distinct variable violations will
occur even though we don't check for them. However, if we just take out
the requirement that $d's on dummies must be explicit, like the
assumption Hmm makes, your approach will be compatible. The spec could
be written for the Hmm approach - i.e. implicit $d's, but still checked
- for clarity. Then your proposal, rather than a spec change, could be
considered an optional faster algorithm for achieving the same goal,
and could be mentioned as a footnote to the spec for future
implementors.</p><p>Aside from historical reasons and inertia, my
resistance to making $d's on dummies optional has to do with my feeling
that this would make the language more confusing to learn. With the
current spec, each proof step can be considered by the user to be a
stand-alone theorem in its own right, with all of its $d requirements
made explicit, without having to reference the final theorem to
determine the implicit $d's of dummy variables (and having to learn
that rule in the first place).</p><p>As you know, on the web pages I
omit any distinct variable groups with dummy variables to make the
pages less cluttered. I made this change a few years ago; before that,
they were explicit like they are in set.mm. While the result is
unquestionably aesthetically more pleasant, and it is unlikely I'd go
back, I had and still have some misgivings because novices have been
confused. The distinct variable concept is already hard enough to
grasp, and in addition we also hit them with (to them) confusing
conditions under which they are "optional", "implicit", or "inferred
from context". Some emails I have received from newcomers have
reflected this confusion. My response and fallback is to <a class="url outside" href="http://us.metamath.org/mpegif/mmset.html#dvnote1">point out</a>
that it's just a display thing for better readability, but "really"
they must be distinct, as the database very explicitly indicates. Their
explicit mention in set.mm provides a kind of security blanket against
being confused by additional rules for implicit $d's. Since they are
not optional, a novice can comment them out to see a detailed error
message explaining exactly what they are used for and why they are
needed.</p><p>Up to now, there was no penalty for requiring the $d's on
dummies to be explicit other than a minor increase in the size of
set.mm. If your proposal results in a significant speedup of
verification, that will be additional impetus for considering a spec
change.</p><p>– <a class="local" href="norm">norm</a> 8 Jan 2007</p><p>Let's
make this discussion more concrete. Suppose the referenced assertion's
replaced variables are A and B, and that there is a mandatory $d
restriction on them.</p><pre>    Case 1: proof step N substitutes
        C and X for A, and
        D and Y for B.

    Case 2: proof step N substitutes
        C and X for A, and
        D and X for B.

    where C and D are mandatory and
          X and Y are optional/dummy.
     </pre><p>Assuming
that condition 1 (no variables in common) is retained and that
condition 2 is modified to restrict the $d inheritance check to <b>just</b> pairs of mandatory variables, then Case 1 is <i>valid</i> and Case 2 is <i>invalid</i> (fails condition 1).</p><p>The big saving in Case 1 is not performing the Optional $d lookup for X/Y, C/Y and D/X.</p><p>However, algorithmically speaking, the  program must now <i>know</i> that X and Y are "dummy" variables in this context – which means doing a lookup to see that <b>both</b>
X and Y have $f hypotheses in the mandatory frame of the theorem being
proved. Unless the "fact" of each variable's mandatory/optional status
is cached somehow, the mandatory $f lookup would need to be performed
for each variable of each possible pair of substituting variables (or,
perhaps subsequent to a NotFound<a class="edit" title="Click to edit this page" href="http://planetx.cc.vt.edu/AsteroidMeta?action=edit;id=NotFound">?</a> result from the mandatory-$d lookup.)</p><p>Thus,
eliminating Optional $d specifications appears to add complexity to the
Proof Verification algorithm but reduces the size of the .mm file.</p><p>A
further Metamath.pdf change would be required in the area describing
Optional Frames. The Optional Frame $d pair lists contain variable
pairs mentioned in $d statements wherein one or both of the variables
do not have mandatory $f hypotheses.</p><dl class="quote"><dt></dt><dd>This
is a good point; metamath.pdf would have to be changed in several
places. I'm not sure the best way to introduce it to the user - we
already have confusing "mandatory" and "optional" things; we would be
adding phantom "implicit" things the user has to imagine in his or her
head. :) – <a class="local" href="norm">norm</a></dd></dl><p>And, making this spec change would trigger code changes in each the <i>n</i>
proof verifiers that have been written over the years… And, in mmj2's
Proof Assistant, for example, the possibly unwise decision was made to
use a "combo" Frame that merges the Mandatory and Optional frames (due
to the fact that new theorems not present in the .mm file can be
proved.) The "combo" frame works precisely because of the parallel
structure of the Mandatory and Optional Frames – it doesn't need to
know which variables or variable pairs are mandatory or optional/dummy.</p><p>--<a class="local" href="ocat">ocat</a> 8-Jan-2007 </p><hr><p><em><a class="local" href="norm">norm</a> here:</em>
Yes, the existing underlying algorithm could affect whether or how much
this algorithm would speed things up. But from a complexity point of
view, if the two expressions being substituted had n variables each,
the number of pairs to be checked could be O(n^2). That's why I would
be interested in empirical speedup figures. Actually, by "Metamath," I
suspect Dan may really mean "Ghilbert" and is using the Metamath spec
for sake of discussion - am I right, Dan? [Later - I see I am right, in
his edit below added simultaneously with mine. – n] In that case, this
discussion would be not be relevant since Ghilbert doesn't require
specifying the dummy $d's. A Ghilbert verifier can choose to use Dan's
method or its existing method, whichever is faster.</p><p>I understand
it would be a big deal to put this into all Metamath verifiers, but
since the existing set.mm is compatible (although not the other way),
it could be phased in slowly. For metamath.exe, I would probably have
both ways implemented - the existing one that requires the presence of
dummy $d's, and Dan's that might speed up the verification. For a long
time, maybe years, I would probably keep set.mm compatible with the old
way, so nothing would have to be changed right away, and there'd be
lots of time to experiment and decide whether it is a good idea. We
could even keep the present spec and just add Dan's method as an
optional "quick verify" for frequent use after set.mm changes, with a
"full verify" (existing algorithm) before a set.mm release that makes
sure all the dummy $d's are present.</p><p>So for mmj2, there is no
hurry. And anyway, I'm not eager to make $d's harder to learn by
introducing the additional concept of hidden "implicit" $d's, unless I
can be convinced that it really isn't more confusing for someone
learning Metamath. To me that is an important consideration, and I
would like to hear opinions on it. – <a class="local" href="norm">norm</a> 8 Jan 2007</p><hr><p>Norm and ocat,</p><p>Thank you very much for your thoughtful and thorough responses to my question!</p><p>I
hadn't actually been coming to this from the point of view of speeding
up verification. I'm presently working on a draft of the Ghilbert
language specification, and I was claiming that non-mandatory distinct
variable pairs were not needed in Ghilbert's equivalent of the active
$d statements for a theorem being proven. However, I was aware that
this was not presently the case for Metamath, and also think there may
be some small differences in how gh_verify.py and shul deal with this
issue, so I wanted to check my understanding of this issue and see if
there was a fundamental (subtle?) reason why the "optional" distinct
variable pairs must be listed.</p><p>I was also wondering if the fact
that Ghilbert may not require the optional distinct variable pairs
while Metamath does might complicate the translation of theorems proved
in Ghilbert back to Metamath, but I don't think that's a serious issue.</p><p>As
to potential speedup, this would of course depend upon the
implementation of the verifier, but I wouldn't expect a big effect one
way or the other.</p><p>– <a class="local" href="dank">dank</a> 8-Jan-2007</p><p>Hi Norm,</p><p>I don't see changing mmj2 as a huge problem. Not at all. Mostly this would be about <i>removing</i> functionality, except for having to "know" that variable <i>v</i>
is a "dummy" during $d checking of each pair of variables. Failure to
modify the code would simply result in erroneous $d error messages… If
this is to be done it would be best to plan it for a given date – say,
3 months in the future. Re-testing everything is the bigger issue, but
some of us have regression test cases :)</p><p>I think <i>learning</i>
Metamath would maybe be simpler without "optional" $d's. (Better to
refer to them as "work variables" than "dummies" – which already has
connotations – or "optional", which doesn't make much sense to a novice
:) Optional Frames are confusing already, along with local $f's --
which otherwise, without them, life would be simpler and we would have
nothing but global variables to worry about.</p><p>I may be kinda slow,
but the whole topic of $d's is tricky without an example of
subject-object language distinctions and maybe an example of mapping
from the .mm subject language to a real object language.</p><p>FYI, I
think converting Metamath to OMDoc is going to be very interesting. I
think the Tarski-Megill approach is novel and worthy of further
research. For example, the use of wff metavariables is like, instead of
First Order Logic, you have 1.5th Order Logic :) The guys doing OMDoc,
MBase, and the Omega System – see mathweb.org – are actually doing the
Asteroid HDM project and they have probably 100 man years invested in
it. Their "web" of provers connects existing provers and uses
distributed processors in a web that spans Europe! Most of it is Lisp,
but they use MySQL<a class="edit" title="Click to edit this page" href="http://planetx.cc.vt.edu/AsteroidMeta?action=edit;id=MySQL">?</a>
and a distributed-concurrent language called Mozart. Why mention this?
Because Metamath is beautiful and elegant – few people can look at
Omega and visually inspect everything to <i>know</i> a proof is
correct (I suppose). On the other hand, the Metamath proof system has
certain rigidities as a result of being logically agnostic
("ontologically uncommitted" as M. Kholhase might say.) Sooooo….I don't
see any interest in converting Metamath to OMDoc/MBase, except for me,
but the conversion process is bound to be theoretically interesting and
will result in, I believe, an addition to the cluster of Logical
Frameworks (each with its own theory of entailment.) Part of the
conversion must be to describe how the Metamath language works,
metalogically with $d's, etc., at a generic level (not just set.mm). So
there will be a new opportunity to <i>explain</i> Metamath, and perhaps even to fit it into the growing OMDoc|MBase| Omega|<a class="local" href="MathWeb">MathWeb</a>
universe. (This is going to be slow trucking for me working alone,
especially because these folk are so far ahead on the HDM front…and
their lead is accelerating, hoho. :) --<a class="local" href="ocat">ocat</a></p><hr><h2>Metamath.pdf 4.3.1, Possible Inaccuracy</h2><p>In the Metamath.pdf section 4.3.1, "The Concept of Unification", sentence 2 of paragraph 2 states:</p><pre>    "The syntax of the Metamath language ensures that if
    a set of substitutions exists, it will be unique."</pre><p>That should probably say something like,</p><pre>    "The unambiguous syntax of set.mm ensures that if a
    set of substitutions exists, it will be unique. The
    author(s) of a Metamath file are solely responsible
    for defining an unambiguous formula syntax.
         </pre><dl class="quote"><dt></dt><dd>I agree that the section may be poorly written, but do you have a specific counterexample in mind? --<a class="local" href="norm">norm</a> 9 Mar 2007<dl class="quote"><dt></dt><dd>Simply delete sentence 2 of paragraph 2 of 4.3.1.<dl class="quote"><dt></dt><dd>I
agree it is misleading, and I'll delete that sentence. It is the
specification, not the syntax, that determines uniqueness of
substitutions. Earlier, on p. 115: "The Metamath language is specified
in such a way that if a set of substitutions exists, it will be unique.
Specifically, the requirement that each variable have a type specified
for it with a $f statement ensures the uniqueness." Historically,
before this requirement was added to the spec, uniqueness was enforced
by the verifier, and the spec said, "a unique substitution must exist"
for a proof to be valid. Now it says (p. 95), "a (unique) substitution
must exist," implying that uniqueness is automatic. In the metamath.exe
program itself, this now-redundant check for uniqueness is still there
(see mmveri.c, line 821, which prints the error message and shows two
of the possible unifications). So if you believe you can construct a
counterexample to uniqueness due to a flaw in the spec, the
metamath.exe verifier (though probably not the others) will detect it.</dd><dt></dt><dd>.</dd><dt></dt><dd>The
uniqueness of unification shouldn't be confused with uniqueness of
parsing, which is a different issue. The latter has to do with whether
there is more than one way to construct the same wff (or class or other
expression type). The philosophy of "pure" Metamath (which, for parsing
simplicity, has been suppressed in the set.mm implementation, thanks to
you) is that a wff construction is just a "proof", and just as there
can be more than one way to prove (2+2)=4, in principle there could be
- and in fact used to be - different ways to show x=y is a wff. – <a class="local" href="norm">norm</a> 10 Mar 2007</dd></dl></dd></dl></dd></dl><p>I
would also suggest an alteration of 4.3.1 to take into account the fact
that a successful unification involves unification of the provable
assertion's formula as well as its floating and essential hypotheses –
though I realize that Metamath.exe may separate these functions
algorithmically.</p><p>For example  (Prepare to ROTFLYAO at the idea of incorporating what follows into Metamath.pdf :-)</p><pre>    'Metamath "unification" is the process of
    determining a consistent set of valid, simultaneous
    substitutions to the variables of an assertion and
    its essential hypotheses such that the resulting
    formulas are identical to the $p assertion formula
    and its essential hypotheses' formulas.

    Definitions:

        "Type" -  the constant, first symbol of a
                  Metamath formula (e.g. "wff", "class"
                  or "set").

        "Expression" - the 2nd through last symbols
                  of a Metamath formula.

        "Valid" - a variable can be validly substituted
                  with any expression whose Type matches
                  the Type of the variable's floating
                  hypothesis formula.

        "Simultaneous" - all substitutions are made at once,
                  which means in effect, that each
                  substitution is independent of every other.
                  For example, given "x + y" and substitutions
                  "x * y" for each "x", and "y * z" for each
                  "y", the resulting expression is
                  "x * y + y * z" -- not "x * y * z + y * z".

        "Consistent" - all occurrences of a given variable
                  in the referenced assertion and its hypotheses
                  must be substituted with the same expression.

    An alternative explanation that may be easier to
    understand is based on the algorithm used in Mel O'Cat's
    mmj2 program -- which has grammatical requirements that
    are slightly stronger than those in metamath.exe, but
    which are satisfied by the grammars of set.mm and ql.mm:

        Suppose that proof step S uses assertion A as
        proof justification (for simplicity assume there
        are no essential hypotheses involved).

        Then, if S and its hypotheses can be unified with A
        and its hypotheses, then proof step S is justified --
        and provable, subject to any disjoint variable
        requirements.

        A Formula S can be "unified" with Formula A if
        the formulas' abstract syntax trees are identical
        except at the variable nodes of Tree A; and

        For each variable node "x" in Tree A, there is
        a subtree "E" at the corresponding tree node
        position of Tree S such that:

            E and x have the same Type; and

            Simultaneous, consistent replacement of subtree
            E for every x node in Tree A is possible.

        If all of these conditions are met then unification
        yields a consistent set of valid, simultaneous
        substitutions that make Tree A equal to Tree S,
        and assertion A can be said to justify proof step S,
        (subject to any disjoint variable restrictions.)
    '
      </pre><p>--<a class="local" href="ocat">ocat</a> 9-Mar-2007</p><h2>An elegant page with a lot of non-latin characters</h2><p>One
of my favourite page: Metamath is cited at the bottom. Characters,
pictures: everything is elegant. It's written in Farsi I suppose.</p><p><a class="url" href="http://fa.wikipedia.org/wiki/%D8%B1%DB%8C%D8%A7%D8%B6%DB%8C%D8%A7%D8%AA">http://fa.wikipedia.org/wiki/%D8%B1%DB%8C%D8%A7%D8%B6%DB%8C%D8%A7%D8%AA</a></p></div><div class="footer"><hr><span class="gotobar bar"><a class="local" href="HomePage">HomePage</a> <a class="local" href="RecentChanges">RecentChanges</a> </span><span class="edit bar"><br> <a class="edit" accesskey="e" title="Click to edit this page" href="http://planetx.cc.vt.edu/AsteroidMeta?action=edit;id=metamathMathQuestions">Edit this page</a> <a class="history" href="http://planetx.cc.vt.edu/AsteroidMeta?action=history;id=metamathMathQuestions">View other revisions</a> <a class="admin" href="http://planetx.cc.vt.edu/AsteroidMeta?action=admin;id=metamathMathQuestions">Administration</a></span><span class="time"><br> Last edited 2007-03-15 08:54 UTC by <a class="author" title="from LPuteaux-151-41-6-248.w217-128.abo.wanadoo.fr" href="fl">fl</a> <a class="diff" href="http://planetx.cc.vt.edu/AsteroidMeta?action=browse;diff=1;id=metamathMathQuestions">(diff)</a></span><form method="get" action="http://planetx.cc.vt.edu/AsteroidMeta" enctype="multipart/form-data">
<p>Search: <input name="search" size="20" accesskey="f" type="text"> <input name="dosearch" value="Go!" type="submit"></p></form></div>
</body></html>
